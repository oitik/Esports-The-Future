{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7918995,"sourceType":"datasetVersion","datasetId":4477853},{"sourceId":9181722,"sourceType":"datasetVersion","datasetId":5528375},{"sourceId":86003,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":72261,"modelId":76277},{"sourceId":11261,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8332,"modelId":3301}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Install packages","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers -U\n#!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:44:54.708938Z","iopub.execute_input":"2024-08-20T07:44:54.709607Z","iopub.status.idle":"2024-08-20T07:46:04.486303Z","shell.execute_reply.started":"2024-08-20T07:44:54.709559Z","shell.execute_reply":"2024-08-20T07:46:04.485202Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers\n  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-yv75840l\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-yv75840l\n  Resolved https://github.com/huggingface/transformers to commit 85345bb439652d3f03bb4e123cef7a440f2ba95b\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.23.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9553698 sha256=62814793b7f35899d87ade56b7e4c97b168b08d6b4b40c662d246cd64ae830ee\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mrzok9rz/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.42.3\n    Uninstalling transformers-4.42.3:\n      Successfully uninstalled transformers-4.42.3\nSuccessfully installed transformers-4.45.0.dev0\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.43.3\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U sentence-transformers\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:46:04.488171Z","iopub.execute_input":"2024-08-20T07:46:04.488490Z","iopub.status.idle":"2024-08-20T07:46:17.682734Z","shell.execute_reply.started":"2024-08-20T07:46:04.488459Z","shell.execute_reply":"2024-08-20T07:46:17.681676Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.0.dev0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.5.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"#!pip install faiss-cpu\n!pip install faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:46:17.684093Z","iopub.execute_input":"2024-08-20T07:46:17.684377Z","iopub.status.idle":"2024-08-20T07:46:33.046718Z","shell.execute_reply.started":"2024-08-20T07:46:17.684349Z","shell.execute_reply":"2024-08-20T07:46:33.045708Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting faiss-gpu\n  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: faiss-gpu\nSuccessfully installed faiss-gpu-1.7.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport ast\n\nimport torch\nimport gc\n\nimport sys, random, string, re, time\nfrom transformers import (BitsAndBytesConfig, \n                          AutoModelForCausalLM, \n                          AutoTokenizer, pipeline)\nfrom tqdm.auto import tqdm\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(f\"CUDA Version: {torch.version.cuda}\")\nprint(f\"Pytorch {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:46:33.049012Z","iopub.execute_input":"2024-08-20T07:46:33.049337Z","iopub.status.idle":"2024-08-20T07:46:52.758512Z","shell.execute_reply.started":"2024-08-20T07:46:33.049290Z","shell.execute_reply":"2024-08-20T07:46:52.757526Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-08-20 07:46:41.495607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-20 07:46:41.495708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-20 07:46:41.658338: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"CUDA Version: 12.1\nPytorch 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:46:52.759519Z","iopub.execute_input":"2024-08-20T07:46:52.760033Z","iopub.status.idle":"2024-08-20T07:46:52.815152Z","shell.execute_reply.started":"2024-08-20T07:46:52.760006Z","shell.execute_reply":"2024-08-20T07:46:52.814123Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Set a seed value\n\nimport torch, random\n\n# Ensure that all GPU operations are deterministic \ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:46:52.816699Z","iopub.execute_input":"2024-08-20T07:46:52.817015Z","iopub.status.idle":"2024-08-20T07:46:52.847790Z","shell.execute_reply.started":"2024-08-20T07:46:52.816988Z","shell.execute_reply":"2024-08-20T07:46:52.846973Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Define Variables","metadata":{}},{"cell_type":"code","source":"# set the path to the Gemma model hosted on Kaggle\nMODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/1\"\nMODEL_PATH_large = \"/kaggle/input/gemma-2/transformers/gemma-2-27b-it/1\"\n\n# set the path to the data that will be used in the few shot prompt\nFEW_SHOT_DATA_PATH = '../input/gemma-comp-data/df_corrected_data.csv'\n\n# set the path the text files containing info about Kaggle\n# KAGGLE_DATA_PATH = '../input/gemma-comp-data/rev4-cleaned-txt-kaggle/'\n\n# the number of results from the vector search that will be reranked\nTOP_K = 20\n\n# the number of text chunks that will be passed to Gemma\nNUM_CHUNKS_IN_CONTEXT = 3","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:36.616187Z","iopub.execute_input":"2024-08-20T07:47:36.616567Z","iopub.status.idle":"2024-08-20T07:47:36.622206Z","shell.execute_reply.started":"2024-08-20T07:47:36.616532Z","shell.execute_reply":"2024-08-20T07:47:36.621326Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Define Device","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"CUDA Version: {torch.version.cuda}\")\nprint(f\"Pytorch {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:38.154944Z","iopub.execute_input":"2024-08-20T07:47:38.155558Z","iopub.status.idle":"2024-08-20T07:47:38.161048Z","shell.execute_reply.started":"2024-08-20T07:47:38.155525Z","shell.execute_reply":"2024-08-20T07:47:38.160110Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Device: cuda\nCUDA Version: 12.1\nPytorch 2.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the type and quantity of GPUs\n\nif torch.cuda.is_available():\n    print('Num CPUs:', os.cpu_count())\n    print('Num GPUs:', torch.cuda.device_count())\n    print('GPU Type:', torch.cuda.get_device_name(0))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:38.842136Z","iopub.execute_input":"2024-08-20T07:47:38.842495Z","iopub.status.idle":"2024-08-20T07:47:38.877588Z","shell.execute_reply.started":"2024-08-20T07:47:38.842464Z","shell.execute_reply":"2024-08-20T07:47:38.876798Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Num CPUs: 4\nNum GPUs: 2\nGPU Type: Tesla T4\n","output_type":"stream"}]},{"cell_type":"code","source":"os.cpu_count()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:39.120780Z","iopub.execute_input":"2024-08-20T07:47:39.121075Z","iopub.status.idle":"2024-08-20T07:47:39.126635Z","shell.execute_reply.started":"2024-08-20T07:47:39.121050Z","shell.execute_reply":"2024-08-20T07:47:39.125783Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"4"},"metadata":{}}]},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"code","source":"def run_faiss_search(query_text, top_k):\n    \n    \"\"\"\n    Executes an exhaustive search using FAISS to find the most \n    similar items to a given query.\n\n    This function vectorizes the input query text using \n    a pre-defined model and then performs a search in a FAISS index \n    to retrieve the top_k most similar items. \n    It returns the indices of these items in the FAISS index, \n    which can be used to retrieve the corresponding documents\n    or items.\n\n    Parameters:\n    - query_text (str): The text of the query for which similar \n    items are to be found.\n    - top_k (int): The number of top similar items to retrieve.\n\n    Returns:\n    - index_vals_list (list of int): A list of indices for the top_k \n    most similar items found in the FAISS index. \n    These indices correspond to the positions of the items in \n    the dataset used to build the FAISS index.\n    \n    Note:\n    - This function assumes that a FAISS index (`faiss_index`) \n    and a model for vectorization (`model`) are already defined \n    outside the function.\n    - The function is designed for use with the Sentence Transformers\n    package to convert text to vectors.\n    \n    \"\"\"\n    \n    # Run FAISS exhaustive search\n    query = [query_text]\n\n    # Vectorize the query string\n    query_embedding = model.encode(query, show_progress_bar=False)\n\n    # Run the query\n    # index_vals refers to the chunk_list index values\n    scores, index_vals = faiss_index.search(query_embedding, top_k)\n    \n    # Get the list of index vals\n    index_vals_list = index_vals[0]\n    \n    return index_vals_list\n    \n\ndef run_rerank(index_vals_list, query_text):\n    \n    \"\"\"\n    Re-ranks a list of retrieved passages based on \n    their similarity to the input query using a cross-encoder.\n\n    This function takes a list of index values corresponding to \n    retrieved passages and the input query text. \n    It then retrieves the actual text of these passages from a \n    dataframe (`df_data`) and formats them for input to a cross-encoder.\n    The cross-encoder is then used to score the similarity between \n    each passage and the query. The passages are re-ranked\n    based on these scores, and the re-ranked list of \n    passages is returned.\n\n    Parameters:\n    - index_vals_list (list of int): A list of index values \n    corresponding to retrieved passages.\n    - query_text (str): The text of the query to be used \n    for re-ranking the passages.\n\n    Returns:\n    - pred_list (list of str): A list of re-ranked passages based \n    on their similarity to the query text.\n\n    Note:\n    - This function assumes that a dataframe (`df_data`) \n    containing the prepared text of passages and a \n    cross-encoder (`cross_encoder`) for scoring the similarity \n    between text pairs are already defined outside the function.\n    \"\"\"\n    \n    # Create a list of text chunks\n    chunk_list = list(df_data['prepared_text'])\n\n    # Replace the chunk index values with the corresponding strings\n    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n\n    # Format the input for the cross encoder\n    # The input to the cross_encoder is a list of lists\n    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n\n    cross_input_list = []\n\n    for item in pred_strings_list:\n        \n        # Create a question/chunk pair: [question, text_chunk]\n        new_list = [query_text, item]\n        \n        # Append to the list containing all the question/chunk pairs\n        # [[question, text_chunk], [question, text_chunk], ...]\n        cross_input_list.append(new_list)\n\n\n    # Put the pred text into a dataframe\n    df = pd.DataFrame(cross_input_list, \n                      columns=['query_text', 'pred_text'])\n\n    # Save the orginal index (i.e. df_data index values)\n    df['original_index'] = index_vals_list\n\n    # Now, score all retrieved passages using the cross_encoder\n    cross_scores = cross_encoder.predict(cross_input_list, show_progress_bar=False)\n\n    # Add the scores to the dataframe\n    df['cross_scores'] = cross_scores\n\n    # Sort the DataFrame in descending order based on the scores\n    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n    \n    # Reset the index\n    df_sorted = df_sorted.reset_index(drop=True)\n\n    pred_list = []\n\n    for i in range(0,len(df_sorted)):\n        \n        # Get the text\n        text = df_sorted.loc[i, 'pred_text']\n        \n        # Add curly braces\n        item = {\n            text\n        }\n\n        # Appen the text to a list\n        pred_list.append(item)\n\n    return pred_list\n\n    \n\ndef vector_search_and_rerank(query_text, top_k=10):\n    \n    \"\"\"\n    Executes a retrieval-augmented generation (RAG) system \n    to generate responses to a given query.\n\n    This function integrates FAISS for initial retrieval and \n    re-ranking using a cross-encoder to produce a list of responses \n    to the input query text. \n    First, it runs a FAISS exhaustive search to retrieve the top_k \n    most relevant passages based on the query. \n    Then, it re-ranks these passages using a cross-encoder\n    to prioritize those with the highest similarity to the query. \n    The resulting list of passages is returned as the \n    output of the RAG system.\n\n    Parameters:\n    - query_text (str): The text of the query for which responses \n    are to be generated.\n    - top_k (int, optional): The number of top passages to \n    retrieve and re-rank. Defaults to 10.\n\n    Returns:\n    - pred_list (list of str): A list of passages ranked and \n    generated by the RAG system in response to the query.\n\n    Note:\n    - This function assumes that `run_faiss_search` and `run_rerank` \n    functions are already defined. \n    These functions handle the initial retrieval and \n    re-ranking processes, respectively.\n    \"\"\"\n    \n    # Run a faiss exhaustive search\n    pred_index_list = run_faiss_search(query_text, top_k)\n\n    # This returns a list of dicts with length equal to top_k\n    pred_list = run_rerank(pred_index_list, query_text)\n    \n    return pred_list\n\n \n\ndef extract_gemma_response(response):\n    \n    # Extract the answer:\n    # Split and select the last item in the list\n    response = response.split('<start_of_turn>model')[-1]\n    # Remove leading and trailing spaces\n    response = response.strip()\n    # Remove the '<end_of_turn> token\n    response = response.replace('<end_of_turn>', \"\")\n\n    # Gemma always uses the phrase \"I cannot answer this question\"\n    # when the answer is not available.\n    text1 = 'I cannot answer this question'\n    \n    # If Gemma can't answer the question then\n    # output a standard response.\n    if text1 in response:\n        response = \"Sorry, that information is not available.\"\n        \n    return response\n\n\ndef format_text(text):\n\n    # Create a list\n    answer_list = text.split('\\n')\n\n    for i, item in enumerate(answer_list):\n\n        # Replace * with nothing\n        new_item = item.replace('*','')\n        \n        # Remove leading and trailing spaces\n        new_item = new_item.strip()\n\n        # Create the output string\n        if i == 0:  \n            fin_string = new_item + '\\n'\n        else:\n            fin_string = fin_string + new_item + '\\n'\n\n    return fin_string\n\n\ndef gemma_assistant(question):\n    \n    # Create the prompt\n    prompt = f\"\"\"<start_of_turn>user \n    Don't use Mardown to format your response.\n    {question}<end_of_turn>\n    <start_of_turn>model\n    \"\"\"\n\n    # Tokenize the prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    # Generate the outputs from prompt\n    generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n    # Decode the generated output\n    generated_text = tokenizer.batch_decode(generate_ids, \n                                        skip_special_tokens=True,\n                                        clean_up_tokenization_spaces=False)[0]\n\n\n    # Extract the answer\n    response = generated_text.split('<start_of_turn>model')[-1]\n    # Remove leading and trailing spaces\n    response = response.strip()\n    # Remove the '<end_of_turn> token\n    response = response.replace('<end_of_turn>', \"\")\n    \n    # Remove markdown '*' symbols\n    response = format_text(response)\n    \n    return response\n\n\ndef timer(start_time):\n\n    # End timing\n    end_time = time.time()\n    # Calculate the elapsed time\n    elapsed_time = end_time - start_time\n    # round to one decimal place\n    elapsed_time = round(elapsed_time, 1)\n    \n    return elapsed_time","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:39.614052Z","iopub.execute_input":"2024-08-20T07:47:39.614376Z","iopub.status.idle":"2024-08-20T07:47:39.635737Z","shell.execute_reply.started":"2024-08-20T07:47:39.614351Z","shell.execute_reply":"2024-08-20T07:47:39.634837Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Initialize Gemma 7b-it\nThere are three important capabilities that LLMs have - knowledge, reasoning and reading comprehension. I experimented with both gemma-2b-it (trained on 2T tokens) and gemma-7b-it (trained on 6T tokens).\n\nI chose the larger gemma-7b-it for this solution because it has a better reasoning ability and better reading comprehension. When both models are given the same reference text and asked to extract an answer to a question, gemma-7b-it more often produced the correct answer.\n\nWe will use the HuggingFace Transformers package to load the model and run inference. We will also use the bitsandbytes package to reduce the size of the model by using 4-bit precision. This will allow it to fit in the memory (RAM) available in this notebook environment.\n\nWe are using two T4 GPUs.\nYou will note that in the code below we have set: device_map=\"auto\"\nThis feature of the Transformers package automatically takes care of of distributing the model across both GPUs.","metadata":{}},{"cell_type":"code","source":"# Initialize the model and the tokenizer.\n# (This step takes about 2 minutes)\n\n\n# Set the compute data type to 16-bit floating point (float16).\n# This is a more memory-efficient format than float32, \n# It lowers memory usage and can speed up computation.\ncompute_dtype = getattr(torch, \"float16\")\n\n\n# Configure the model to use 4-bit precision for certain weights, \n# and specify the quantization details. This further reduces the \n# model size and can speed up inference.\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n)\n\n# Load the causal language model with the defined quantization \n# configuration and set it to automatically map \n# to the available device.\ngemma_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\n                                        device_map=\"auto\",\n                                        quantization_config=bnb_config)\n\n# Disable caching of past key values for transformer models.\n# This reduces memory usage in scenarios where past key values \n# aren't needed for subsequent predictions.\ngemma_model.config.use_cache = False\n\n# Set the pretraining throughput to 1.\ngemma_model.config.pretraining_tp = 1\n\n# Initialize the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:47:40.033760Z","iopub.execute_input":"2024-08-20T07:47:40.034527Z","iopub.status.idle":"2024-08-20T07:49:48.137370Z","shell.execute_reply.started":"2024-08-20T07:47:40.034496Z","shell.execute_reply":"2024-08-20T07:49:48.136513Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\nGemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n`config.hidden_activation` if you want to override this behaviour.\nSee https://github.com/huggingface/transformers/pull/29402 for more details.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"977408457bb0430cb46b527a473857c0"}},"metadata":{}}]},{"cell_type":"code","source":"gemma_model.config","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:49:48.138832Z","iopub.execute_input":"2024-08-20T07:49:48.139141Z","iopub.status.idle":"2024-08-20T07:49:48.146582Z","shell.execute_reply.started":"2024-08-20T07:49:48.139116Z","shell.execute_reply":"2024-08-20T07:49:48.145786Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"GemmaConfig {\n  \"_name_or_path\": \"/kaggle/input/gemma/transformers/7b-it/1\",\n  \"architectures\": [\n    \"GemmaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 1,\n  \"head_dim\": 256,\n  \"hidden_act\": \"gelu\",\n  \"hidden_activation\": \"gelu_pytorch_tanh\",\n  \"hidden_size\": 3072,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 24576,\n  \"max_position_embeddings\": 8192,\n  \"model_type\": \"gemma\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 16,\n  \"pad_token_id\": 0,\n  \"pretraining_tp\": 1,\n  \"quantization_config\": {\n    \"_load_in_4bit\": true,\n    \"_load_in_8bit\": false,\n    \"bnb_4bit_compute_dtype\": \"float16\",\n    \"bnb_4bit_quant_storage\": \"uint8\",\n    \"bnb_4bit_quant_type\": \"nf4\",\n    \"bnb_4bit_use_double_quant\": false,\n    \"llm_int8_enable_fp32_cpu_offload\": false,\n    \"llm_int8_has_fp16_weight\": false,\n    \"llm_int8_skip_modules\": null,\n    \"llm_int8_threshold\": 6.0,\n    \"load_in_4bit\": true,\n    \"load_in_8bit\": false,\n    \"quant_method\": \"bitsandbytes\"\n  },\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 10000.0,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.45.0.dev0\",\n  \"use_cache\": false,\n  \"vocab_size\": 256000\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Ask Gemma questions about Some Questions\n\nLet's ask Gemma a few questions about esports. Gemma would have gained this knowledge during training.\n\nIt's important to use a good prompt template when working with Gemma. If we don't then we might get bad outputs. The prompt template we will be using is explained here:\nhttps://www.promptingguide.ai/models/gemma","metadata":{}},{"cell_type":"code","source":"ques = \"What is Esports?\"\n\n# Create a Prompt\nprompt = f\"\"\"<start_of_turn>user\n{ques}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n# prompt\n# Start timing\nstart_time = time.time()\n\n## Tokenize the prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n\ngenerate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n\n# DEcode the generated output\ngenerated_text = tokenizer.batch_decode(generate_ids, \n                                    skip_special_tokens=True,\n                                    clean_up_tokenization_spaces=False)[0]\n\n# Extract the answer\n\n# Split and select the last item in the list\nresponse = generated_text.split('<start_of_turn>model')[-1]\n# Remove leading and trailing spaces\nresponse = response.strip()\n# Remove the '<end_of_turn> token\nresponse = response.replace('<end_of_turn>', \"\")\n\n# Remove markdown '*' symbols\n# The deafult Markdown that Gemma outputs\n# doesn't always display well.\nresponse = format_text(response)\n\n\n# Get the inference time\nelapsed_time = timer(start_time)\nprint(f\"Time taken: {elapsed_time} seconds\")\n\nprint()\nprint('User:\\n',ques)\nprint()\nprint('Gemma:\\n', response)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:49:56.483027Z","iopub.execute_input":"2024-08-20T07:49:56.483860Z","iopub.status.idle":"2024-08-20T07:50:00.767947Z","shell.execute_reply.started":"2024-08-20T07:49:56.483828Z","shell.execute_reply":"2024-08-20T07:50:00.767101Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Time taken: 4.3 seconds\n\nUser:\n What is Esports?\n\nGemma:\n Esports is a term used to describe competitive video gaming events. It is a relatively new term, but it has become increasingly popular in recent years. Esports events are held all over the world, and they can be watched by millions of people.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start timing\nstart_time = time.time()\n\nquestion = \"What is the condition of esports in Bangladesh?\"\n\nanswer = gemma_assistant(question)\n\n\n# Get the inference time\nelapsed_time = timer(start_time)\nprint(f\"Time taken: {elapsed_time} seconds\")\nprint()\n\nprint('User:\\n',question)\nprint()\nprint('Gemma:\\n',answer)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:50:09.241838Z","iopub.execute_input":"2024-08-20T07:50:09.242438Z","iopub.status.idle":"2024-08-20T07:50:30.326350Z","shell.execute_reply.started":"2024-08-20T07:50:09.242405Z","shell.execute_reply":"2024-08-20T07:50:30.325253Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Time taken: 21.1 seconds\n\nUser:\n What is the condition of esports in Bangladesh?\n\nGemma:\n The esports landscape in Bangladesh is burgeoning, albeit still in its early stages. Here's a breakdown of its current condition:\n\nGrowth:\n\nIncreased Participation: The number of esports players in Bangladesh is growing rapidly, with estimates reaching up to 2 million.\nRising Popularity: Esports viewership is gaining traction, with large audiences tuning in to watch local tournaments and international competitions.\nGovernment Support: The Bangladesh government has recognized the potential of esports and has begun to invest in its development, including allocating funds for infrastructure and training programs.\n\nChallenges:\n\nLack of Infrastructure: Bangladesh faces challenges in terms of infrastructure, with limited access to high-speed internet and proper gaming equipment.\nFinancial Barriers: Many aspiring esports players in Bangladesh face financial difficulties, making it difficult to afford equipment and travel costs for tournaments.\nLimited Opportunities: Compared to other countries, Bangladesh has fewer opportunities for esports professionals, limiting career paths for top players.\n\nFuture Potential:\n\nDespite the challenges, the future of esports in Bangladesh is promising. With increasing participation, rising popularity, and government support, the country has the potential to become a major force in the global esports scene. Additionally, the growing talent pool of young players and the increasing focus on esports education are creating a positive outlook for the future.\n\nOverall, the condition of esports in Bangladesh is on an upward trajectory. While there are challenges to overcome, the potential for growth and development is high. With continued support and investment, Bangladesh has the potential to become a major player in the global esports landscape.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Start timing\nstart_time = time.time()\n\nquestion = \"Name one esports organization from Bangladesh.\"\n\nanswer = gemma_assistant(question)\n\n\n# Get the inference time\nelapsed_time = timer(start_time)\nprint(f\"Time taken: {elapsed_time} seconds\")\nprint()\n\nprint('User:\\n',question)\nprint()\nprint('Gemma:\\n',answer)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:52:54.774284Z","iopub.execute_input":"2024-08-20T07:52:54.775280Z","iopub.status.idle":"2024-08-20T07:52:56.379049Z","shell.execute_reply.started":"2024-08-20T07:52:54.775244Z","shell.execute_reply":"2024-08-20T07:52:56.378115Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Time taken: 1.6 seconds\n\nUser:\n Name one esports organization from Bangladesh.\n\nGemma:\n The answer is Team Souq.\n\nTeam Souq is an esports organization based in Bangladesh.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"question = \"Who is prime minister of Bangladesh?\"\n\nanswer = gemma_assistant(question)\n\nprint('User:\\n',question)\nprint()\nprint('Gemma:\\n',answer)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:53:00.405701Z","iopub.execute_input":"2024-08-20T07:53:00.406058Z","iopub.status.idle":"2024-08-20T07:53:01.406295Z","shell.execute_reply.started":"2024-08-20T07:53:00.406030Z","shell.execute_reply":"2024-08-20T07:53:01.404861Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"User:\n Who is prime minister of Bangladesh?\n\nGemma:\n The Prime Minister of Bangladesh is Sheikh Hasina.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Read the Data Set","metadata":{}},{"cell_type":"code","source":"df = pd.read_parquet('/kaggle/input/esports-data/live_comments_2/bangladesh_livechat_data.parquet')\n\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:53:06.174226Z","iopub.execute_input":"2024-08-20T07:53:06.174930Z","iopub.status.idle":"2024-08-20T07:53:09.172338Z","shell.execute_reply.started":"2024-08-20T07:53:06.174896Z","shell.execute_reply":"2024-08-20T07:53:09.171427Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"              datetime      author message     video_id\n0  2022-08-12 09:22:22      Rizvan     hii  oiqpD3C_dLo\n1  2022-08-12 09:22:35      Rizvan     #2d  oiqpD3C_dLo\n2  2022-08-12 09:25:29  RN KAKASHI     3rd  oiqpD3C_dLo","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>author</th>\n      <th>message</th>\n      <th>video_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-08-12 09:22:22</td>\n      <td>Rizvan</td>\n      <td>hii</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-08-12 09:22:35</td>\n      <td>Rizvan</td>\n      <td>#2d</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-08-12 09:25:29</td>\n      <td>RN KAKASHI</td>\n      <td>3rd</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"context = \"\"\" I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\n\nA1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\nFS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\nThese organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\n\"\"\"\n\ncontext","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:53:11.189217Z","iopub.execute_input":"2024-08-20T07:53:11.189588Z","iopub.status.idle":"2024-08-20T07:53:11.196601Z","shell.execute_reply.started":"2024-08-20T07:53:11.189557Z","shell.execute_reply":"2024-08-20T07:53:11.195590Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"\" I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\\n\\nA1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\\nFS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\\nThese organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\\n\""},"metadata":{}}]},{"cell_type":"code","source":"ques = \"Name some Esports organization from Bangladesh.\"","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:53:18.221923Z","iopub.execute_input":"2024-08-20T07:53:18.222652Z","iopub.status.idle":"2024-08-20T07:53:18.226710Z","shell.execute_reply.started":"2024-08-20T07:53:18.222617Z","shell.execute_reply":"2024-08-20T07:53:18.225726Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Create the prompt\nprompt = f\"\"\"<start_of_turn>user\nContext: {context}\nQuestion: {ques}<end_of_turn>\n<start_of_turn>model\n\"\"\"\n\n    \n# Tokenize the prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n# Generate the outputs from prompt\ngenerate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n# Decode the generated output\nresponse = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n                                     clean_up_tokenization_spaces=False)[0]\n\n\n# Extract the answer\n\n# Split and select the last item in the list\ngemma_response = response.split('<start_of_turn>model')[-1]\n# Remove leading and trailing spaces\ngemma_response = gemma_response.strip()\n# Remove the '<end_of_turn> token\ngemma_response= gemma_response.replace('<end_of_turn>', \"\")\n\n# Get the inference time\nelapsed_time = timer(start_time)\nprint(f\"Time taken: {elapsed_time} seconds\")\nprint()\n\nprint('-----')\nprint('User:\\n',ques)\nprint()\nprint('Raw Gemma response:\\n\\n',response)\nprint()\nprint()\nprint('Extracted Gemma response:\\n\\n',gemma_response)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T07:53:20.312104Z","iopub.execute_input":"2024-08-20T07:53:20.312480Z","iopub.status.idle":"2024-08-20T07:53:22.606736Z","shell.execute_reply.started":"2024-08-20T07:53:20.312450Z","shell.execute_reply":"2024-08-20T07:53:22.605852Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Time taken: 27.8 seconds\n\n-----\nUser:\n Name some Esports organization from Bangladesh.\n\nRaw Gemma response:\n\n <start_of_turn>user\nContext:  I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\n\nA1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\nFS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\nThese organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\n\nQuestion: Name some Esports organization from Bangladesh.<end_of_turn>\n<start_of_turn>model\nSure, here are the named Esports organization from Bangladesh in the text:\n\n- A1 Esports\n- FS-Gaming (FsGSM)\n\n\nExtracted Gemma response:\n\n Sure, here are the named Esports organization from Bangladesh in the text:\n\n- A1 Esports\n- FS-Gaming (FsGSM)\n","output_type":"stream"}]},{"cell_type":"code","source":"def hello_world(question, context):   \n    # Create the prompt\n    prompt = f\"\"\"<start_of_turn>user\n    Context: {context}\n    Question: {question}<end_of_turn>\n    <start_of_turn>model\n    \"\"\"\n\n\n    # Tokenize the prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    # Generate the outputs from prompt\n    generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n    # Decode the generated output\n    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n                                         clean_up_tokenization_spaces=False)[0]\n\n\n    # Extract the answer\n\n    # Split and select the last item in the list\n    gemma_response = response.split('<start_of_turn>model')[-1]\n    # Remove leading and trailing spaces\n    gemma_response = gemma_response.strip()\n    # Remove the '<end_of_turn> token\n    gemma_response= gemma_response.replace('<end_of_turn>', \"\")\n    \n    # Clear the memory to create space\n    del prompt\n    del inputs\n    del generate_ids\n    torch.cuda.empty_cache() \n    gc.collect()\n        \n    return gemma_response","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:01:49.620685Z","iopub.execute_input":"2024-08-20T08:01:49.621475Z","iopub.status.idle":"2024-08-20T08:01:49.628401Z","shell.execute_reply.started":"2024-08-20T08:01:49.621442Z","shell.execute_reply":"2024-08-20T08:01:49.627456Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"context = f\"\"\"\nCOVID-19, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly became a global pandemic. Characterized by respiratory symptoms, fever, and fatigue, the virus spread across the world, leading to millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n\"\"\"\nques = \"Whaat is covid 19?\"\n\nprint(context)\nprint(ques)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:01:56.762799Z","iopub.execute_input":"2024-08-20T08:01:56.763162Z","iopub.status.idle":"2024-08-20T08:01:56.769010Z","shell.execute_reply.started":"2024-08-20T08:01:56.763131Z","shell.execute_reply":"2024-08-20T08:01:56.768129Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\nCOVID-19, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly became a global pandemic. Characterized by respiratory symptoms, fever, and fatigue, the virus spread across the world, leading to millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n\nWhaat is covid 19?\n","output_type":"stream"}]},{"cell_type":"code","source":"ans = hello_world(ques, context)\nprint(ques)\n\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:00.891979Z","iopub.execute_input":"2024-08-20T08:02:00.892634Z","iopub.status.idle":"2024-08-20T08:02:10.596606Z","shell.execute_reply.started":"2024-08-20T08:02:00.892602Z","shell.execute_reply":"2024-08-20T08:02:10.595684Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"Whaat is covid 19?\nSure, here is the answer to the question:\n\nCOVID-19, caused by the SARS-CoV-2 virus, is a viral infection that emerged in late 2019 and quickly spread worldwide, becoming a global pandemic. It is characterized by respiratory symptoms, fever, and fatigue. The virus has caused millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n","output_type":"stream"}]},{"cell_type":"code","source":"ques = \"Give three topics from this context. Not any Description Just the topics.\"\nans = hello_world(ques, context)\nprint(ques)\n\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:13.041680Z","iopub.execute_input":"2024-08-20T08:02:13.042028Z","iopub.status.idle":"2024-08-20T08:02:16.858887Z","shell.execute_reply.started":"2024-08-20T08:02:13.041998Z","shell.execute_reply":"2024-08-20T08:02:16.857955Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Give three topics from this context. Not any Description Just the topics.\nSure, here are the three topics from the provided text:\n\n    1. COVID-19 pandemic and its impact on global health\n    2. Vaccination efforts and public health measures\n    3. The importance of public health infrastructure\n","output_type":"stream"}]},{"cell_type":"code","source":"context_text = f\"\"\"\nA mother is often seen as the heart of the family, offering unconditional love, care, and support. She nurtures and guides her children, providing them with the foundation they need to grow into confident and compassionate individuals. A mother's role goes beyond just caregiving; she imparts values, offers wisdom, and serves as a source of comfort and strength throughout her children's lives. Her influence is profound, shaping not only the lives of her children but also the future of the family.\n\"\"\"\nans = hello_world(ques, context_text)\nprint(ques)\n\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:19.248119Z","iopub.execute_input":"2024-08-20T08:02:19.248905Z","iopub.status.idle":"2024-08-20T08:02:23.072773Z","shell.execute_reply.started":"2024-08-20T08:02:19.248871Z","shell.execute_reply":"2024-08-20T08:02:23.071883Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Give three topics from this context. Not any Description Just the topics.\nSure, here are the three topics from the provided text:\n\n    1. The role of mothers in the family\n    2. The impact of mothers on their children\n    3. The influence of mothers on the future of the family\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Use the Chat Data","metadata":{}},{"cell_type":"code","source":"df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:30.060482Z","iopub.execute_input":"2024-08-20T08:02:30.061332Z","iopub.status.idle":"2024-08-20T08:02:30.071659Z","shell.execute_reply.started":"2024-08-20T08:02:30.061290Z","shell.execute_reply":"2024-08-20T08:02:30.070689Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"              datetime      author message     video_id\n0  2022-08-12 09:22:22      Rizvan     hii  oiqpD3C_dLo\n1  2022-08-12 09:22:35      Rizvan     #2d  oiqpD3C_dLo\n2  2022-08-12 09:25:29  RN KAKASHI     3rd  oiqpD3C_dLo","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>datetime</th>\n      <th>author</th>\n      <th>message</th>\n      <th>video_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-08-12 09:22:22</td>\n      <td>Rizvan</td>\n      <td>hii</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-08-12 09:22:35</td>\n      <td>Rizvan</td>\n      <td>#2d</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-08-12 09:25:29</td>\n      <td>RN KAKASHI</td>\n      <td>3rd</td>\n      <td>oiqpD3C_dLo</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import emoji\n\ndef replace_emoji_codes(text):\n    \"\"\"\n    Replace text-based emoji codes with actual emojis using the `emoji` library.\n\n    Parameters:\n    - text (str): The input text containing emoji codes.\n\n    Returns:\n    - str: The text with emoji codes replaced by actual emojis.\n    \"\"\"\n    # Convert text-based emoji codes to actual emojis\n    return emoji.emojize(text)\n\n# Example usage\ntext = \"A1 :red_heart: A1 :red_heart:\"\nconverted_text = replace_emoji_codes(text)\nprint(converted_text)  # Output: \"A1 ❤️ A1 ❤️\"","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:30.782968Z","iopub.execute_input":"2024-08-20T08:02:30.783802Z","iopub.status.idle":"2024-08-20T08:02:30.839111Z","shell.execute_reply.started":"2024-08-20T08:02:30.783768Z","shell.execute_reply":"2024-08-20T08:02:30.838270Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"A1 ❤️ A1 ❤️\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\ntqdm.pandas()\ndf['message'] = df.message.progress_apply(replace_emoji_codes)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:02:32.181732Z","iopub.execute_input":"2024-08-20T08:02:32.182086Z","iopub.status.idle":"2024-08-20T08:03:02.405180Z","shell.execute_reply.started":"2024-08-20T08:02:32.182057Z","shell.execute_reply":"2024-08-20T08:03:02.404426Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"100%|██████████| 2796193/2796193 [00:30<00:00, 92743.68it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"len(df)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:03:23.800542Z","iopub.execute_input":"2024-08-20T08:03:23.800907Z","iopub.status.idle":"2024-08-20T08:03:23.808367Z","shell.execute_reply.started":"2024-08-20T08:03:23.800874Z","shell.execute_reply":"2024-08-20T08:03:23.807509Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"2796193"},"metadata":{}}]},{"cell_type":"code","source":"# filt = df[:10].copy()\n\nfkchat = df['message']\n\ndemo1 = fkchat[:100]\nmsg = \", \".join(demo1.astype(str))\nmsg","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:03:33.261063Z","iopub.execute_input":"2024-08-20T08:03:33.261442Z","iopub.status.idle":"2024-08-20T08:03:33.268769Z","shell.execute_reply.started":"2024-08-20T08:03:33.261408Z","shell.execute_reply":"2024-08-20T08:03:33.267809Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"'hii, #2d, 3rd, Soul🤮🤮🤮🤮🤮🤮, 🚀🚀🚀, A, A1, 4th, still 1 hour, st op, SouL, STE, STE, ste, soul🤣🤣🤣🤣🤣, rsg, Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣 Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣, Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣 Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣Soul se. nehi ho payega 🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣🤣, hlw, opp, ste,love from A1 , RSG too op, vai India me pubg ban he eslea soul ke ping high he 😭😭😭🤫😂 INDIA people SAY in 8R tournament, vai India me pubg ban he eslea soul ke ping high he 😭😭😭🤫😂 INDIA people SAY in 8R tournament, when you can,t win Make Excuse😅😅, love from Bangladesh , Soul Love from Bangladesh  , RSG opppppppppppppppppppp, a1, oppp, dhol🤣, a1 bosti😂😂😂, jonathan❤️❤️ a1 bosti😂, ❌. 👍, LOVE FROM A1 SINISTER PLEASE FC, A1 sinister always op, A1 sinister always OP, Sonathon always best kamasutra, 💜💜💜, nv go, 🥰🥰, Lets go ste 🔥, nv, 😂😂, NV go, ste❤️, STE, Nv champion hobe 🤘, NV Love from Bangladesh , STE 🖤🖤🖤, OhiO BB, Nova, DRS, T2K ❤️, ALTER EGO 🥰 suuuiii:cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling::cat-orange-whistling:, st, ste💖💖💖💖💖, Ste, Any Bangladeshi team here??, no, bruh, no there is no bd teams, nova bruh, go ste, #STEftw, #STEftw, #STEftw,   ste, #STEftw,   , ste ftw, ste ftw, ste, STENV, play game, 4am, NOVA, NOVA, MAGNITUDE CINEMATIC GAMING NEPAL, nova, nova, nova, nova, , , how much time, A1🙂💔💔💔💔💔💔💔💔💔💔💔💔, যদি এখন এইখানে বাংলাদেশের একটা দল থাকতো🙂🙂💔💔💔💔💔💔💔, lets gooooooooooo, :eyes-purple-crying:, NOVA❤️, NOVA❤️, Nova gg💜💜, STE🖤, STE|IHC 💜, ste love from , NV and STE, miss A1 😅💔🥺, STE, STE,, nova❤️❤️,  vs 🐍, A1❤️A1'"},"metadata":{}}]},{"cell_type":"code","source":"context = msg\n# print(context)\n\nques = \"These are some live Chats from a Esports Live. Now Give me 2 short key topics from these chats. No Description just the topic please. Nothing else just the two topic name.\"\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:04:23.372112Z","iopub.execute_input":"2024-08-20T08:04:23.372480Z","iopub.status.idle":"2024-08-20T08:04:23.376674Z","shell.execute_reply.started":"2024-08-20T08:04:23.372449Z","shell.execute_reply":"2024-08-20T08:04:23.375721Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"ans = hello_world(ques, context)\n\nprint(ques, end='\\n')\n\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:04:23.703099Z","iopub.execute_input":"2024-08-20T08:04:23.703870Z","iopub.status.idle":"2024-08-20T08:04:27.340189Z","shell.execute_reply.started":"2024-08-20T08:04:23.703839Z","shell.execute_reply":"2024-08-20T08:04:27.339181Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"These are some live Chats from a Esports Live. Now Give me 2 short key topics from these chats. No Description just the topic please. Nothing else just the two topic name.\nSure, here are the two key topics from the live chat:\n\n    - Soul se. nehi ho payega 🤣\n    - Ste and A1 sinister\n","output_type":"stream"}]},{"cell_type":"code","source":"x = 100\ndemo1 = fkchat[x:x+100]\nmsg = \", \".join(demo1.astype(str))\nmsg","metadata":{"execution":{"iopub.status.busy":"2024-08-19T18:06:20.342676Z","iopub.execute_input":"2024-08-19T18:06:20.343079Z","iopub.status.idle":"2024-08-19T18:06:20.350458Z","shell.execute_reply.started":"2024-08-19T18:06:20.343046Z","shell.execute_reply":"2024-08-19T18:06:20.349430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = msg\nans = hello_world(ques, context)\n\nprint(ques, end='\\n')\n\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T18:06:43.516428Z","iopub.execute_input":"2024-08-19T18:06:43.517091Z","iopub.status.idle":"2024-08-19T18:06:46.384928Z","shell.execute_reply.started":"2024-08-19T18:06:43.517059Z","shell.execute_reply":"2024-08-19T18:06:46.383973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = 200\ndemo1 = fkchat[x:x+100]\nmsg = \", \".join(demo1.astype(str))\ncontext = msg\nans = hello_world(ques, context)\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T18:10:44.694909Z","iopub.execute_input":"2024-08-19T18:10:44.695266Z","iopub.status.idle":"2024-08-19T18:10:47.751448Z","shell.execute_reply.started":"2024-08-19T18:10:44.695239Z","shell.execute_reply":"2024-08-19T18:10:47.750289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Apply it in a Loop","metadata":{}},{"cell_type":"code","source":"x = 0\nchunk_size = 100\nresults = []\n\n# lim = len(fkchat)\nlim = 1000\n\ntotal_iterations = (lim - x) // chunk_size\n\n# Loop through the DataFrame in chunks of 100 with a progress bar\nfor _ in tqdm(range(total_iterations), desc=\"Processing\"):\n    # Get the next 100 messages\n    demo1 = fkchat[x:x + chunk_size]\n    \n    # Convert the messages to a single string, separated by commas\n    msg = \", \".join(demo1.astype(str))\n    \n    # Pass the context to the hello_world function and store the result\n    ans = hello_world(ques, msg)\n    lines = ans.splitlines()\n\n    # Rejoin the lines, skipping the first one\n    new_text = \"\\n\".join(lines[1:])\n\n    results.append(new_text)\n    \n    # Move to the next chunk\n    x += chunk_size","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:04:47.169277Z","iopub.execute_input":"2024-08-20T08:04:47.169917Z","iopub.status.idle":"2024-08-20T08:05:19.458867Z","shell.execute_reply.started":"2024-08-20T08:04:47.169883Z","shell.execute_reply":"2024-08-20T08:05:19.457844Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"Processing: 100%|██████████| 10/10 [00:32<00:00,  3.23s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"# for v in results:\n#     print(v)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:16.965764Z","iopub.execute_input":"2024-08-20T08:06:16.966102Z","iopub.status.idle":"2024-08-20T08:06:16.970404Z","shell.execute_reply.started":"2024-08-20T08:06:16.966077Z","shell.execute_reply":"2024-08-20T08:06:16.969365Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"results[0]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:17.482801Z","iopub.execute_input":"2024-08-20T08:06:17.483142Z","iopub.status.idle":"2024-08-20T08:06:17.489039Z","shell.execute_reply.started":"2024-08-20T08:06:17.483113Z","shell.execute_reply":"2024-08-20T08:06:17.488136Z"},"trusted":true},"execution_count":47,"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"'\\n    - Soul se. nehi ho payega 🤣\\n    - Ste and A1 sinister'"},"metadata":{}}]},{"cell_type":"code","source":"import re\n\ntext = \"\\n    - Soul se. nehi ho payega 🤣\\n    - Ste and A1 sinister\"\n\n# Split the text into separate lines based on newline\nseparated_texts = [line.strip() for line in text.split('\\n') if line.strip()]\n\n# Remove leading hyphens, numbers, or other unwanted characters\ncleaned_texts = [re.sub(r'^[\\d\\-\\s]+', '', line) for line in separated_texts]\n\n# Now you have cleaned lines as separate values in a list\nprint(cleaned_texts)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:20.286359Z","iopub.execute_input":"2024-08-20T08:06:20.287000Z","iopub.status.idle":"2024-08-20T08:06:20.293019Z","shell.execute_reply.started":"2024-08-20T08:06:20.286969Z","shell.execute_reply":"2024-08-20T08:06:20.292026Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"['Soul se. nehi ho payega 🤣', 'Ste and A1 sinister']\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport string\n\ndef cln_texts(text):\n    # Split the text into separate lines based on newline\n    separated_texts = [line.strip() for line in text.split('\\n') if line.strip()]\n\n    # Remove leading hyphens, numbers, or other unwanted characters\n    cleaned_texts = [re.sub(r'^[\\d\\-\\s]+', '', line) for line in separated_texts]\n\n    # Remove punctuation\n    cleaned_texts = [line.translate(str.maketrans('', '', string.punctuation)) for line in cleaned_texts]\n    \n    # Remove extra spaces\n    cleaned_texts = [re.sub(r'\\s+', ' ', line).strip() for line in cleaned_texts]\n    \n    return cleaned_texts","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:20.512490Z","iopub.execute_input":"2024-08-20T08:06:20.512972Z","iopub.status.idle":"2024-08-20T08:06:20.519103Z","shell.execute_reply.started":"2024-08-20T08:06:20.512946Z","shell.execute_reply":"2024-08-20T08:06:20.518150Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"farr = []\nfor v in tqdm(results):\n    farr.append(cln_texts(v))\n    \nfarr = [item for sublist in farr for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:21.636578Z","iopub.execute_input":"2024-08-20T08:06:21.637278Z","iopub.status.idle":"2024-08-20T08:06:21.645589Z","shell.execute_reply.started":"2024-08-20T08:06:21.637248Z","shell.execute_reply":"2024-08-20T08:06:21.644689Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:00<00:00, 20078.05it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"farr","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:24.601507Z","iopub.execute_input":"2024-08-20T08:06:24.602248Z","iopub.status.idle":"2024-08-20T08:06:24.608034Z","shell.execute_reply.started":"2024-08-20T08:06:24.602216Z","shell.execute_reply":"2024-08-20T08:06:24.607149Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"['Soul se nehi ho payega 🤣',\n 'Ste and A1 sinister',\n 'Team Spirit and Fan Love',\n 'Gaming Community and Support',\n 'A1 fan support and love',\n 'Ste and A1 fan love',\n 'Match koytay',\n 'Soul and A1 related topics',\n 'A1 Esports',\n 'Corona Virus',\n 'Esports News and Discussion',\n 'Personal and Gaming Discussions',\n 'Champion Ste',\n 'Bangladesh Khali Hingsa Kore Jabe',\n 'Indias victory in the World Cup',\n 'The ongoing game and its excitement',\n 'Gaming and Esports',\n 'Social Interaction and Entertainment',\n 'Gaming and Esports',\n 'Bangladeshi Team and Fan Engagement']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Now Apply for big chunks","metadata":{}},{"cell_type":"code","source":"x = 0\nchunk_size = 100\nult_results = []\n\n# lim = len(fkchat)\nlim = 10000\n\ntotal_iterations = (lim - x) // chunk_size\n\n# Loop through the DataFrame in chunks of 100 with a progress bar\nfor _ in tqdm(range(total_iterations), desc=\"Processing\"):\n    # Get the next 100 messages\n    demo1 = fkchat[x:x + chunk_size]\n    \n    # Convert the messages to a single string, separated by commas\n    msg = \", \".join(demo1.astype(str))\n    \n    # Pass the context to the hello_world function and store the result\n    ans = hello_world(ques, msg)\n    lines = ans.splitlines()\n\n    # Rejoin the lines, skipping the first one\n    new_text = \"\\n\".join(lines[1:])\n\n    ult_results.append(new_text)\n    \n    # Move to the next chunk\n    x += chunk_size","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:06:46.848722Z","iopub.execute_input":"2024-08-20T08:06:46.849609Z","iopub.status.idle":"2024-08-20T08:12:46.874901Z","shell.execute_reply.started":"2024-08-20T08:06:46.849574Z","shell.execute_reply":"2024-08-20T08:12:46.873998Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stderr","text":"Processing: 100%|██████████| 100/100 [06:00<00:00,  3.60s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"farr = []\nfor v in tqdm(ult_results):\n    farr.append(cln_texts(v))\n    \nfarr = [item for sublist in farr for item in sublist]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:13:16.608464Z","iopub.execute_input":"2024-08-20T08:13:16.609055Z","iopub.status.idle":"2024-08-20T08:13:16.619557Z","shell.execute_reply.started":"2024-08-20T08:13:16.609022Z","shell.execute_reply":"2024-08-20T08:13:16.618622Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"100%|██████████| 100/100 [00:00<00:00, 36986.81it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"farr[:10]","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:26:17.716718Z","iopub.execute_input":"2024-08-20T08:26:17.717393Z","iopub.status.idle":"2024-08-20T08:26:17.723050Z","shell.execute_reply.started":"2024-08-20T08:26:17.717363Z","shell.execute_reply":"2024-08-20T08:26:17.722168Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"['Soul se nehi ho payega 🤣',\n 'Ste and A1 sinister',\n 'Team Spirit and Fan Love',\n 'Gaming Community and Support',\n 'A1 fan support and love',\n 'Ste and A1 fan love',\n 'Match koytay',\n 'Soul and A1 related topics',\n 'A1 Esports',\n 'Corona Virus']"},"metadata":{}}]},{"cell_type":"markdown","source":"## Shorten The Topic List","metadata":{}},{"cell_type":"code","source":"topics = \", \".join(farr)\n# topics","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:37:40.611718Z","iopub.execute_input":"2024-08-20T08:37:40.612096Z","iopub.status.idle":"2024-08-20T08:37:40.617226Z","shell.execute_reply.started":"2024-08-20T08:37:40.612050Z","shell.execute_reply":"2024-08-20T08:37:40.616299Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"ques = \"There are similar type of topics in this context list. From these topics give me top 5 topics.\"","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:35:28.617356Z","iopub.execute_input":"2024-08-20T08:35:28.617704Z","iopub.status.idle":"2024-08-20T08:35:28.621895Z","shell.execute_reply.started":"2024-08-20T08:35:28.617660Z","shell.execute_reply":"2024-08-20T08:35:28.621031Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"ans = hello_world(ques, topics)\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T08:37:43.832082Z","iopub.execute_input":"2024-08-20T08:37:43.832482Z","iopub.status.idle":"2024-08-20T08:37:48.863318Z","shell.execute_reply.started":"2024-08-20T08:37:43.832452Z","shell.execute_reply":"2024-08-20T08:37:48.862352Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"Sure, here are the top 5 topics from the provided text:\n\n1. Team discussions and strategies\n2. Fan engagement and community building\n3. Gaming and Esports\n4. Personal and Social Discussions\n5. Esports News and Updates\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Few Shot Prompting","metadata":{}},{"cell_type":"code","source":"# Load the few shot data into a pandas dataframe\ndf_fshot = pd.read_csv(FEW_SHOT_DATA_PATH)\n\ndef convert_to_list(x):\n    \n    # Convert the string to a list: '[...]' to [...]\n    x_as_list = ast.literal_eval(x)\n    \n    return x_as_list\n\n# Convert each item in the context column from a string to a \n# python list i.e. '[...]' to [...]\ndf_fshot['gem_context'] = df_fshot['gem_context'].apply(convert_to_list)\n\ndf_fshot.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:11:31.889602Z","iopub.execute_input":"2024-08-19T17:11:31.889940Z","iopub.status.idle":"2024-08-19T17:11:31.924516Z","shell.execute_reply.started":"2024-08-19T17:11:31.889915Z","shell.execute_reply":"2024-08-19T17:11:31.923607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df_fshot.shape)\ndf_fshot.tail(3)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T16:26:14.445469Z","iopub.execute_input":"2024-08-19T16:26:14.445823Z","iopub.status.idle":"2024-08-19T16:26:14.465677Z","shell.execute_reply.started":"2024-08-19T16:26:14.445792Z","shell.execute_reply":"2024-08-19T16:26:14.464596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hello_world_again(question, context):   \n    # Create the prompt\n    prompt = f\"\"\"<start_of_turn>user\n    Context: {df_fshot.loc[0, 'gem_context']}\n    Question: {df_fshot.loc[0, 'query']}<end_of_turn>\n    <start_of_turn>model\n    {df_fshot.loc[0, 'corrected_text']}<end_of_turn>\n    <start_of_turn>user\n    Context: {df_fshot.loc[5, 'gem_context']}\n    Question: {df_fshot.loc[5, 'query']}<end_of_turn>\n    <start_of_turn>model\n    {df_fshot.loc[5, 'corrected_text']}<end_of_turn>\n    <start_of_turn>user\n    Context: {df_fshot.loc[6, 'gem_context']}\n    Question: {df_fshot.loc[6, 'query']}<end_of_turn>\n    <start_of_turn>model\n    {df_fshot.loc[6, 'corrected_text']}<end_of_turn>\n    <start_of_turn>user\n    Think and write your step-by-step reasoning before responding.\n    \n    Context: {context}\n    Question: {question}<end_of_turn>\n    <start_of_turn>model\n    \"\"\"\n\n\n    # Tokenize the prompt\n    inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n    # Generate the outputs from prompt\n    generate_ids = gemma_model.generate(**inp, max_new_tokens=768)\n    # Decode the generated output\n    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n                                         clean_up_tokenization_spaces=False)[0]\n\n\n    # Extract the answer\n\n    # Split and select the last item in the list\n    gemma_response = response.split('<start_of_turn>model')[-1]\n    # Remove leading and trailing spaces\n    gemma_response = gemma_response.strip()\n    # Remove the '<end_of_turn> token\n    gemma_response= gemma_response.replace('<end_of_turn>', \"\")\n    \n    return gemma_response","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:16:42.720034Z","iopub.execute_input":"2024-08-19T17:16:42.720404Z","iopub.status.idle":"2024-08-19T17:16:42.728198Z","shell.execute_reply.started":"2024-08-19T17:16:42.720376Z","shell.execute_reply":"2024-08-19T17:16:42.727300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = hello_world_again(ques, context)\nprint(ques)\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:16:57.067935Z","iopub.execute_input":"2024-08-19T17:16:57.068339Z","iopub.status.idle":"2024-08-19T17:17:00.987523Z","shell.execute_reply.started":"2024-08-19T17:16:57.068311Z","shell.execute_reply":"2024-08-19T17:17:00.986607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans = hello_world_again(ques, context_text)\nprint(ques)\nprint(ans)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:17:46.400111Z","iopub.execute_input":"2024-08-19T17:17:46.400962Z","iopub.status.idle":"2024-08-19T17:17:50.915038Z","shell.execute_reply.started":"2024-08-19T17:17:46.400929Z","shell.execute_reply":"2024-08-19T17:17:50.914061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}