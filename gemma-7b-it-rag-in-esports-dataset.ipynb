{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850ddc25",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.019623,
     "end_time": "2024-08-20T19:17:43.011209",
     "exception": false,
     "start_time": "2024-08-20T19:17:42.991586",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d16e9d0",
   "metadata": {
    "papermill": {
     "duration": 0.019601,
     "end_time": "2024-08-20T19:17:43.049713",
     "exception": false,
     "start_time": "2024-08-20T19:17:43.030112",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34ca988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:17:43.089480Z",
     "iopub.status.busy": "2024-08-20T19:17:43.088762Z",
     "iopub.status.idle": "2024-08-20T19:18:53.139926Z",
     "shell.execute_reply": "2024-08-20T19:18:53.138839Z"
    },
    "papermill": {
     "duration": 70.073766,
     "end_time": "2024-08-20T19:18:53.142276",
     "exception": false,
     "start_time": "2024-08-20T19:17:43.068510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/huggingface/transformers\r\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-vgust7nt\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-vgust7nt\r\n",
      "  Resolved https://github.com/huggingface/transformers to commit c63a3d0f1791e018de447ac570fc7029d1ea19bd\r\n",
      "  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (3.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.23.4)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (6.0.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2023.12.25)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (0.4.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.45.0.dev0) (4.66.4)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (2024.5.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0.dev0) (4.9.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.45.0.dev0) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.45.0.dev0) (2024.7.4)\r\n",
      "Building wheels for collected packages: transformers\r\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for transformers: filename=transformers-4.45.0.dev0-py3-none-any.whl size=9553550 sha256=9588e60d20b4dd4d35fdd6232b4ae91335b299f27b0c553aff36a16678fd5f61\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-duh7voml/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\r\n",
      "Successfully built transformers\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.42.3\r\n",
      "    Uninstalling transformers-4.42.3:\r\n",
      "      Successfully uninstalled transformers-4.42.3\r\n",
      "Successfully installed transformers-4.45.0.dev0\r\n",
      "Looking in indexes: https://pypi.org/simple/\r\n",
      "Collecting bitsandbytes\r\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\r\n",
      "Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\r\n",
      "Successfully installed bitsandbytes-0.43.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers -U\n",
    "#!pip install accelerate\n",
    "!pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706fc831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:18:53.199792Z",
     "iopub.status.busy": "2024-08-20T19:18:53.199026Z",
     "iopub.status.idle": "2024-08-20T19:19:06.449576Z",
     "shell.execute_reply": "2024-08-20T19:19:06.448676Z"
    },
    "papermill": {
     "duration": 13.281924,
     "end_time": "2024-08-20T19:19:06.451793",
     "exception": false,
     "start_time": "2024-08-20T19:18:53.169869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\r\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.0.dev0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.26.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.23.4)\r\n",
      "Requirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.5.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.1.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\r\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentence-transformers\r\n",
      "Successfully installed sentence-transformers-3.0.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf60fa4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:06.510937Z",
     "iopub.status.busy": "2024-08-20T19:19:06.510232Z",
     "iopub.status.idle": "2024-08-20T19:19:22.694646Z",
     "shell.execute_reply": "2024-08-20T19:19:22.693462Z"
    },
    "papermill": {
     "duration": 16.216037,
     "end_time": "2024-08-20T19:19:22.697021",
     "exception": false,
     "start_time": "2024-08-20T19:19:06.480984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-gpu\r\n",
      "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\r\n",
      "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faiss-gpu\r\n",
      "Successfully installed faiss-gpu-1.7.2\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install faiss-cpu\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947c2e7",
   "metadata": {
    "papermill": {
     "duration": 0.031582,
     "end_time": "2024-08-20T19:19:22.760913",
     "exception": false,
     "start_time": "2024-08-20T19:19:22.729331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e113650",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:22.825718Z",
     "iopub.status.busy": "2024-08-20T19:19:22.825122Z",
     "iopub.status.idle": "2024-08-20T19:19:44.555657Z",
     "shell.execute_reply": "2024-08-20T19:19:44.554652Z"
    },
    "papermill": {
     "duration": 21.765384,
     "end_time": "2024-08-20T19:19:44.557693",
     "exception": false,
     "start_time": "2024-08-20T19:19:22.792309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-20 19:19:32.550394: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-20 19:19:32.550494: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-20 19:19:32.706601: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 12.1\n",
      "Pytorch 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "import sys, random, string, re, time\n",
    "from transformers import (BitsAndBytesConfig, \n",
    "                          AutoModelForCausalLM, \n",
    "                          AutoTokenizer, pipeline)\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Don't Show Warning Messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Pytorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41db520a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:44.622864Z",
     "iopub.status.busy": "2024-08-20T19:19:44.622323Z",
     "iopub.status.idle": "2024-08-20T19:19:44.713017Z",
     "shell.execute_reply": "2024-08-20T19:19:44.712108Z"
    },
    "papermill": {
     "duration": 0.125087,
     "end_time": "2024-08-20T19:19:44.714958",
     "exception": false,
     "start_time": "2024-08-20T19:19:44.589871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcdddcde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:44.780459Z",
     "iopub.status.busy": "2024-08-20T19:19:44.780146Z",
     "iopub.status.idle": "2024-08-20T19:19:44.788093Z",
     "shell.execute_reply": "2024-08-20T19:19:44.787435Z"
    },
    "papermill": {
     "duration": 0.042415,
     "end_time": "2024-08-20T19:19:44.789909",
     "exception": false,
     "start_time": "2024-08-20T19:19:44.747494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set a seed value\n",
    "\n",
    "import torch, random\n",
    "\n",
    "# Ensure that all GPU operations are deterministic \n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a340ae2",
   "metadata": {
    "papermill": {
     "duration": 0.031561,
     "end_time": "2024-08-20T19:19:44.853448",
     "exception": false,
     "start_time": "2024-08-20T19:19:44.821887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32afd3a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:44.958898Z",
     "iopub.status.busy": "2024-08-20T19:19:44.958543Z",
     "iopub.status.idle": "2024-08-20T19:19:44.963340Z",
     "shell.execute_reply": "2024-08-20T19:19:44.962468Z"
    },
    "papermill": {
     "duration": 0.079983,
     "end_time": "2024-08-20T19:19:44.965174",
     "exception": false,
     "start_time": "2024-08-20T19:19:44.885191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the path to the Gemma model hosted on Kaggle\n",
    "MODEL_PATH = \"/kaggle/input/gemma/transformers/7b-it/1\"\n",
    "MODEL_PATH_large = \"/kaggle/input/gemma-2/transformers/gemma-2-27b-it/1\"\n",
    "\n",
    "# set the path to the data that will be used in the few shot prompt\n",
    "FEW_SHOT_DATA_PATH = '../input/gemma-comp-data/df_corrected_data.csv'\n",
    "\n",
    "# set the path the text files containing info about Kaggle\n",
    "# KAGGLE_DATA_PATH = '../input/gemma-comp-data/rev4-cleaned-txt-kaggle/'\n",
    "\n",
    "# the number of results from the vector search that will be reranked\n",
    "TOP_K = 20\n",
    "\n",
    "# the number of text chunks that will be passed to Gemma\n",
    "NUM_CHUNKS_IN_CONTEXT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777bb6d",
   "metadata": {
    "papermill": {
     "duration": 0.032009,
     "end_time": "2024-08-20T19:19:45.028962",
     "exception": false,
     "start_time": "2024-08-20T19:19:44.996953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Define Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a667a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:45.093773Z",
     "iopub.status.busy": "2024-08-20T19:19:45.093440Z",
     "iopub.status.idle": "2024-08-20T19:19:45.098875Z",
     "shell.execute_reply": "2024-08-20T19:19:45.097875Z"
    },
    "papermill": {
     "duration": 0.040151,
     "end_time": "2024-08-20T19:19:45.100750",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.060599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA Version: 12.1\n",
      "Pytorch 2.1.2\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"Pytorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dff3cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:45.165786Z",
     "iopub.status.busy": "2024-08-20T19:19:45.165510Z",
     "iopub.status.idle": "2024-08-20T19:19:45.200166Z",
     "shell.execute_reply": "2024-08-20T19:19:45.199189Z"
    },
    "papermill": {
     "duration": 0.069062,
     "end_time": "2024-08-20T19:19:45.202048",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.132986",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num CPUs: 4\n",
      "Num GPUs: 2\n",
      "GPU Type: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Check the type and quantity of GPUs\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('Num CPUs:', os.cpu_count())\n",
    "    print('Num GPUs:', torch.cuda.device_count())\n",
    "    print('GPU Type:', torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4805ef0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:45.268558Z",
     "iopub.status.busy": "2024-08-20T19:19:45.268082Z",
     "iopub.status.idle": "2024-08-20T19:19:45.273365Z",
     "shell.execute_reply": "2024-08-20T19:19:45.272536Z"
    },
    "papermill": {
     "duration": 0.040337,
     "end_time": "2024-08-20T19:19:45.275303",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.234966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16153f1e",
   "metadata": {
    "papermill": {
     "duration": 0.031849,
     "end_time": "2024-08-20T19:19:45.339198",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.307349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d463d27c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:45.406924Z",
     "iopub.status.busy": "2024-08-20T19:19:45.406643Z",
     "iopub.status.idle": "2024-08-20T19:19:45.428151Z",
     "shell.execute_reply": "2024-08-20T19:19:45.427309Z"
    },
    "papermill": {
     "duration": 0.057322,
     "end_time": "2024-08-20T19:19:45.430077",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.372755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_faiss_search(query_text, top_k):\n",
    "    \n",
    "    \"\"\"\n",
    "    Executes an exhaustive search using FAISS to find the most \n",
    "    similar items to a given query.\n",
    "\n",
    "    This function vectorizes the input query text using \n",
    "    a pre-defined model and then performs a search in a FAISS index \n",
    "    to retrieve the top_k most similar items. \n",
    "    It returns the indices of these items in the FAISS index, \n",
    "    which can be used to retrieve the corresponding documents\n",
    "    or items.\n",
    "\n",
    "    Parameters:\n",
    "    - query_text (str): The text of the query for which similar \n",
    "    items are to be found.\n",
    "    - top_k (int): The number of top similar items to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - index_vals_list (list of int): A list of indices for the top_k \n",
    "    most similar items found in the FAISS index. \n",
    "    These indices correspond to the positions of the items in \n",
    "    the dataset used to build the FAISS index.\n",
    "    \n",
    "    Note:\n",
    "    - This function assumes that a FAISS index (`faiss_index`) \n",
    "    and a model for vectorization (`model`) are already defined \n",
    "    outside the function.\n",
    "    - The function is designed for use with the Sentence Transformers\n",
    "    package to convert text to vectors.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Run FAISS exhaustive search\n",
    "    query = [query_text]\n",
    "\n",
    "    # Vectorize the query string\n",
    "    query_embedding = model.encode(query, show_progress_bar=False)\n",
    "\n",
    "    # Run the query\n",
    "    # index_vals refers to the chunk_list index values\n",
    "    scores, index_vals = faiss_index.search(query_embedding, top_k)\n",
    "    \n",
    "    # Get the list of index vals\n",
    "    index_vals_list = index_vals[0]\n",
    "    \n",
    "    return index_vals_list\n",
    "    \n",
    "\n",
    "def run_rerank(index_vals_list, query_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    Re-ranks a list of retrieved passages based on \n",
    "    their similarity to the input query using a cross-encoder.\n",
    "\n",
    "    This function takes a list of index values corresponding to \n",
    "    retrieved passages and the input query text. \n",
    "    It then retrieves the actual text of these passages from a \n",
    "    dataframe (`df_data`) and formats them for input to a cross-encoder.\n",
    "    The cross-encoder is then used to score the similarity between \n",
    "    each passage and the query. The passages are re-ranked\n",
    "    based on these scores, and the re-ranked list of \n",
    "    passages is returned.\n",
    "\n",
    "    Parameters:\n",
    "    - index_vals_list (list of int): A list of index values \n",
    "    corresponding to retrieved passages.\n",
    "    - query_text (str): The text of the query to be used \n",
    "    for re-ranking the passages.\n",
    "\n",
    "    Returns:\n",
    "    - pred_list (list of str): A list of re-ranked passages based \n",
    "    on their similarity to the query text.\n",
    "\n",
    "    Note:\n",
    "    - This function assumes that a dataframe (`df_data`) \n",
    "    containing the prepared text of passages and a \n",
    "    cross-encoder (`cross_encoder`) for scoring the similarity \n",
    "    between text pairs are already defined outside the function.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a list of text chunks\n",
    "    chunk_list = list(df_data['prepared_text'])\n",
    "\n",
    "    # Replace the chunk index values with the corresponding strings\n",
    "    pred_strings_list = [chunk_list[item] for item in index_vals_list]\n",
    "\n",
    "    # Format the input for the cross encoder\n",
    "    # The input to the cross_encoder is a list of lists\n",
    "    # [[query_text, pred_text1], [query_text, pred_text2], ...]\n",
    "\n",
    "    cross_input_list = []\n",
    "\n",
    "    for item in pred_strings_list:\n",
    "        \n",
    "        # Create a question/chunk pair: [question, text_chunk]\n",
    "        new_list = [query_text, item]\n",
    "        \n",
    "        # Append to the list containing all the question/chunk pairs\n",
    "        # [[question, text_chunk], [question, text_chunk], ...]\n",
    "        cross_input_list.append(new_list)\n",
    "\n",
    "\n",
    "    # Put the pred text into a dataframe\n",
    "    df = pd.DataFrame(cross_input_list, \n",
    "                      columns=['query_text', 'pred_text'])\n",
    "\n",
    "    # Save the orginal index (i.e. df_data index values)\n",
    "    df['original_index'] = index_vals_list\n",
    "\n",
    "    # Now, score all retrieved passages using the cross_encoder\n",
    "    cross_scores = cross_encoder.predict(cross_input_list, show_progress_bar=False)\n",
    "\n",
    "    # Add the scores to the dataframe\n",
    "    df['cross_scores'] = cross_scores\n",
    "\n",
    "    # Sort the DataFrame in descending order based on the scores\n",
    "    df_sorted = df.sort_values(by='cross_scores', ascending=False)\n",
    "    \n",
    "    # Reset the index\n",
    "    df_sorted = df_sorted.reset_index(drop=True)\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for i in range(0,len(df_sorted)):\n",
    "        \n",
    "        # Get the text\n",
    "        text = df_sorted.loc[i, 'pred_text']\n",
    "        \n",
    "        # Add curly braces\n",
    "        item = {\n",
    "            text\n",
    "        }\n",
    "\n",
    "        # Appen the text to a list\n",
    "        pred_list.append(item)\n",
    "\n",
    "    return pred_list\n",
    "\n",
    "    \n",
    "\n",
    "def vector_search_and_rerank(query_text, top_k=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    Executes a retrieval-augmented generation (RAG) system \n",
    "    to generate responses to a given query.\n",
    "\n",
    "    This function integrates FAISS for initial retrieval and \n",
    "    re-ranking using a cross-encoder to produce a list of responses \n",
    "    to the input query text. \n",
    "    First, it runs a FAISS exhaustive search to retrieve the top_k \n",
    "    most relevant passages based on the query. \n",
    "    Then, it re-ranks these passages using a cross-encoder\n",
    "    to prioritize those with the highest similarity to the query. \n",
    "    The resulting list of passages is returned as the \n",
    "    output of the RAG system.\n",
    "\n",
    "    Parameters:\n",
    "    - query_text (str): The text of the query for which responses \n",
    "    are to be generated.\n",
    "    - top_k (int, optional): The number of top passages to \n",
    "    retrieve and re-rank. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    - pred_list (list of str): A list of passages ranked and \n",
    "    generated by the RAG system in response to the query.\n",
    "\n",
    "    Note:\n",
    "    - This function assumes that `run_faiss_search` and `run_rerank` \n",
    "    functions are already defined. \n",
    "    These functions handle the initial retrieval and \n",
    "    re-ranking processes, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run a faiss exhaustive search\n",
    "    pred_index_list = run_faiss_search(query_text, top_k)\n",
    "\n",
    "    # This returns a list of dicts with length equal to top_k\n",
    "    pred_list = run_rerank(pred_index_list, query_text)\n",
    "    \n",
    "    return pred_list\n",
    "\n",
    " \n",
    "\n",
    "def extract_gemma_response(response):\n",
    "    \n",
    "    # Extract the answer:\n",
    "    # Split and select the last item in the list\n",
    "    response = response.split('<start_of_turn>model')[-1]\n",
    "    # Remove leading and trailing spaces\n",
    "    response = response.strip()\n",
    "    # Remove the '<end_of_turn> token\n",
    "    response = response.replace('<end_of_turn>', \"\")\n",
    "\n",
    "    # Gemma always uses the phrase \"I cannot answer this question\"\n",
    "    # when the answer is not available.\n",
    "    text1 = 'I cannot answer this question'\n",
    "    \n",
    "    # If Gemma can't answer the question then\n",
    "    # output a standard response.\n",
    "    if text1 in response:\n",
    "        response = \"Sorry, that information is not available.\"\n",
    "        \n",
    "    return response\n",
    "\n",
    "\n",
    "def format_text(text):\n",
    "\n",
    "    # Create a list\n",
    "    answer_list = text.split('\\n')\n",
    "\n",
    "    for i, item in enumerate(answer_list):\n",
    "\n",
    "        # Replace * with nothing\n",
    "        new_item = item.replace('*','')\n",
    "        \n",
    "        # Remove leading and trailing spaces\n",
    "        new_item = new_item.strip()\n",
    "\n",
    "        # Create the output string\n",
    "        if i == 0:  \n",
    "            fin_string = new_item + '\\n'\n",
    "        else:\n",
    "            fin_string = fin_string + new_item + '\\n'\n",
    "\n",
    "    return fin_string\n",
    "\n",
    "\n",
    "def gemma_assistant(question):\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"<start_of_turn>user \n",
    "    Don't use Mardown to format your response.\n",
    "    {question}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    # Generate the outputs from prompt\n",
    "    generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.batch_decode(generate_ids, \n",
    "                                        skip_special_tokens=True,\n",
    "                                        clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "    # Extract the answer\n",
    "    response = generated_text.split('<start_of_turn>model')[-1]\n",
    "    # Remove leading and trailing spaces\n",
    "    response = response.strip()\n",
    "    # Remove the '<end_of_turn> token\n",
    "    response = response.replace('<end_of_turn>', \"\")\n",
    "    \n",
    "    # Remove markdown '*' symbols\n",
    "    response = format_text(response)\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def timer(start_time):\n",
    "\n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    # Calculate the elapsed time\n",
    "    elapsed_time = end_time - start_time\n",
    "    # round to one decimal place\n",
    "    elapsed_time = round(elapsed_time, 1)\n",
    "    \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4fcff",
   "metadata": {
    "papermill": {
     "duration": 0.031746,
     "end_time": "2024-08-20T19:19:45.493903",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.462157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Initialize Gemma 7b-it\n",
    "There are three important capabilities that LLMs have - knowledge, reasoning and reading comprehension. I experimented with both gemma-2b-it (trained on 2T tokens) and gemma-7b-it (trained on 6T tokens).\n",
    "\n",
    "I chose the larger gemma-7b-it for this solution because it has a better reasoning ability and better reading comprehension. When both models are given the same reference text and asked to extract an answer to a question, gemma-7b-it more often produced the correct answer.\n",
    "\n",
    "We will use the HuggingFace Transformers package to load the model and run inference. We will also use the bitsandbytes package to reduce the size of the model by using 4-bit precision. This will allow it to fit in the memory (RAM) available in this notebook environment.\n",
    "\n",
    "We are using two T4 GPUs.\n",
    "You will note that in the code below we have set: device_map=\"auto\"\n",
    "This feature of the Transformers package automatically takes care of of distributing the model across both GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3ccf7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:19:45.562735Z",
     "iopub.status.busy": "2024-08-20T19:19:45.562381Z",
     "iopub.status.idle": "2024-08-20T19:21:41.076939Z",
     "shell.execute_reply": "2024-08-20T19:21:41.075950Z"
    },
    "papermill": {
     "duration": 115.553282,
     "end_time": "2024-08-20T19:21:41.079273",
     "exception": false,
     "start_time": "2024-08-20T19:19:45.525991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1dbaff1845a4fdfa8c23d7e7b3af47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the model and the tokenizer.\n",
    "# (This step takes about 2 minutes)\n",
    "\n",
    "\n",
    "# Set the compute data type to 16-bit floating point (float16).\n",
    "# This is a more memory-efficient format than float32, \n",
    "# It lowers memory usage and can speed up computation.\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "\n",
    "# Configure the model to use 4-bit precision for certain weights, \n",
    "# and specify the quantization details. This further reduces the \n",
    "# model size and can speed up inference.\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# Load the causal language model with the defined quantization \n",
    "# configuration and set it to automatically map \n",
    "# to the available device.\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        device_map=\"auto\",\n",
    "                                        quantization_config=bnb_config)\n",
    "\n",
    "# Disable caching of past key values for transformer models.\n",
    "# This reduces memory usage in scenarios where past key values \n",
    "# aren't needed for subsequent predictions.\n",
    "gemma_model.config.use_cache = False\n",
    "\n",
    "# Set the pretraining throughput to 1.\n",
    "gemma_model.config.pretraining_tp = 1\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "763da798",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:41.147399Z",
     "iopub.status.busy": "2024-08-20T19:21:41.146818Z",
     "iopub.status.idle": "2024-08-20T19:21:41.154262Z",
     "shell.execute_reply": "2024-08-20T19:21:41.153353Z"
    },
    "papermill": {
     "duration": 0.043196,
     "end_time": "2024-08-20T19:21:41.156071",
     "exception": false,
     "start_time": "2024-08-20T19:21:41.112875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaConfig {\n",
       "  \"_name_or_path\": \"/kaggle/input/gemma/transformers/7b-it/1\",\n",
       "  \"architectures\": [\n",
       "    \"GemmaForCausalLM\"\n",
       "  ],\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 2,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"head_dim\": 256,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_activation\": \"gelu_pytorch_tanh\",\n",
       "  \"hidden_size\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 24576,\n",
       "  \"max_position_embeddings\": 8192,\n",
       "  \"model_type\": \"gemma\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 28,\n",
       "  \"num_key_value_heads\": 16,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"quantization_config\": {\n",
       "    \"_load_in_4bit\": true,\n",
       "    \"_load_in_8bit\": false,\n",
       "    \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "    \"bnb_4bit_quant_type\": \"nf4\",\n",
       "    \"bnb_4bit_use_double_quant\": false,\n",
       "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "    \"llm_int8_has_fp16_weight\": false,\n",
       "    \"llm_int8_skip_modules\": null,\n",
       "    \"llm_int8_threshold\": 6.0,\n",
       "    \"load_in_4bit\": true,\n",
       "    \"load_in_8bit\": false,\n",
       "    \"quant_method\": \"bitsandbytes\"\n",
       "  },\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.45.0.dev0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 256000\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemma_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1ff6a0",
   "metadata": {
    "papermill": {
     "duration": 0.032151,
     "end_time": "2024-08-20T19:21:41.221337",
     "exception": false,
     "start_time": "2024-08-20T19:21:41.189186",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ask Gemma questions about Some Questions\n",
    "\n",
    "Let's ask Gemma a few questions about esports. Gemma would have gained this knowledge during training.\n",
    "\n",
    "It's important to use a good prompt template when working with Gemma. If we don't then we might get bad outputs. The prompt template we will be using is explained here:\n",
    "https://www.promptingguide.ai/models/gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "614ad78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:41.288023Z",
     "iopub.status.busy": "2024-08-20T19:21:41.287727Z",
     "iopub.status.idle": "2024-08-20T19:21:45.647651Z",
     "shell.execute_reply": "2024-08-20T19:21:45.646669Z"
    },
    "papermill": {
     "duration": 4.395494,
     "end_time": "2024-08-20T19:21:45.649848",
     "exception": false,
     "start_time": "2024-08-20T19:21:41.254354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 4.4 seconds\n",
      "\n",
      "User:\n",
      " What is Esports?\n",
      "\n",
      "Gemma:\n",
      " Esports is a term used to describe competitive video gaming events. It is a relatively new term, but it has become increasingly popular in recent years. Esports events are held all over the world, and they can be watched by millions of people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ques = \"What is Esports?\"\n",
    "\n",
    "# Create a Prompt\n",
    "prompt = f\"\"\"<start_of_turn>user\n",
    "{ques}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "# prompt\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "## Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n",
    "\n",
    "# DEcode the generated output\n",
    "generated_text = tokenizer.batch_decode(generate_ids, \n",
    "                                    skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "# Extract the answer\n",
    "\n",
    "# Split and select the last item in the list\n",
    "response = generated_text.split('<start_of_turn>model')[-1]\n",
    "# Remove leading and trailing spaces\n",
    "response = response.strip()\n",
    "# Remove the '<end_of_turn> token\n",
    "response = response.replace('<end_of_turn>', \"\")\n",
    "\n",
    "# Remove markdown '*' symbols\n",
    "# The deafult Markdown that Gemma outputs\n",
    "# doesn't always display well.\n",
    "response = format_text(response)\n",
    "\n",
    "\n",
    "# Get the inference time\n",
    "elapsed_time = timer(start_time)\n",
    "print(f\"Time taken: {elapsed_time} seconds\")\n",
    "\n",
    "print()\n",
    "print('User:\\n',ques)\n",
    "print()\n",
    "print('Gemma:\\n', response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac314fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:45.718005Z",
     "iopub.status.busy": "2024-08-20T19:21:45.717306Z",
     "iopub.status.idle": "2024-08-20T19:21:45.721466Z",
     "shell.execute_reply": "2024-08-20T19:21:45.720648Z"
    },
    "papermill": {
     "duration": 0.039866,
     "end_time": "2024-08-20T19:21:45.723275",
     "exception": false,
     "start_time": "2024-08-20T19:21:45.683409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# question = \"What is the condition of esports in Bangladesh?\"\n",
    "\n",
    "# answer = gemma_assistant(question)\n",
    "\n",
    "\n",
    "# # Get the inference time\n",
    "# elapsed_time = timer(start_time)\n",
    "# print(f\"Time taken: {elapsed_time} seconds\")\n",
    "# print()\n",
    "\n",
    "# print('User:\\n',question)\n",
    "# print()\n",
    "# print('Gemma:\\n',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35668b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:45.791004Z",
     "iopub.status.busy": "2024-08-20T19:21:45.790518Z",
     "iopub.status.idle": "2024-08-20T19:21:45.794505Z",
     "shell.execute_reply": "2024-08-20T19:21:45.793594Z"
    },
    "papermill": {
     "duration": 0.040167,
     "end_time": "2024-08-20T19:21:45.796338",
     "exception": false,
     "start_time": "2024-08-20T19:21:45.756171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start timing\n",
    "# start_time = time.time()\n",
    "\n",
    "# question = \"Name one esports organization from Bangladesh.\"\n",
    "\n",
    "# answer = gemma_assistant(question)\n",
    "\n",
    "\n",
    "# # Get the inference time\n",
    "# elapsed_time = timer(start_time)\n",
    "# print(f\"Time taken: {elapsed_time} seconds\")\n",
    "# print()\n",
    "\n",
    "# print('User:\\n',question)\n",
    "# print()\n",
    "# print('Gemma:\\n',answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734a33ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:45.866173Z",
     "iopub.status.busy": "2024-08-20T19:21:45.865879Z",
     "iopub.status.idle": "2024-08-20T19:21:45.869735Z",
     "shell.execute_reply": "2024-08-20T19:21:45.868952Z"
    },
    "papermill": {
     "duration": 0.040523,
     "end_time": "2024-08-20T19:21:45.871579",
     "exception": false,
     "start_time": "2024-08-20T19:21:45.831056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# question = \"Who is prime minister of Bangladesh?\"\n",
    "\n",
    "# answer = gemma_assistant(question)\n",
    "\n",
    "# print('User:\\n',question)\n",
    "# print()\n",
    "# print('Gemma:\\n',answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240d47d",
   "metadata": {
    "papermill": {
     "duration": 0.034068,
     "end_time": "2024-08-20T19:21:45.938785",
     "exception": false,
     "start_time": "2024-08-20T19:21:45.904717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Read the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c38d0f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:46.006321Z",
     "iopub.status.busy": "2024-08-20T19:21:46.005543Z",
     "iopub.status.idle": "2024-08-20T19:21:48.820001Z",
     "shell.execute_reply": "2024-08-20T19:21:48.819060Z"
    },
    "papermill": {
     "duration": 2.850644,
     "end_time": "2024-08-20T19:21:48.822132",
     "exception": false,
     "start_time": "2024-08-20T19:21:45.971488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>author</th>\n",
       "      <th>message</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-12 09:22:22</td>\n",
       "      <td>Rizvan</td>\n",
       "      <td>hii</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-12 09:22:35</td>\n",
       "      <td>Rizvan</td>\n",
       "      <td>#2d</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-12 09:25:29</td>\n",
       "      <td>RN KAKASHI</td>\n",
       "      <td>3rd</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      author message     video_id\n",
       "0  2022-08-12 09:22:22      Rizvan     hii  oiqpD3C_dLo\n",
       "1  2022-08-12 09:22:35      Rizvan     #2d  oiqpD3C_dLo\n",
       "2  2022-08-12 09:25:29  RN KAKASHI     3rd  oiqpD3C_dLo"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('/kaggle/input/esports-data/live_comments_2/bangladesh_livechat_data.parquet')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33d177f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:48.896665Z",
     "iopub.status.busy": "2024-08-20T19:21:48.895888Z",
     "iopub.status.idle": "2024-08-20T19:21:48.902497Z",
     "shell.execute_reply": "2024-08-20T19:21:48.901656Z"
    },
    "papermill": {
     "duration": 0.045436,
     "end_time": "2024-08-20T19:21:48.904472",
     "exception": false,
     "start_time": "2024-08-20T19:21:48.859036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\\n\\nA1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\\nFS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\\nThese organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = \"\"\" I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\n",
    "\n",
    "A1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\n",
    "FS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\n",
    "These organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\n",
    "\"\"\"\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b761a65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:48.973067Z",
     "iopub.status.busy": "2024-08-20T19:21:48.972789Z",
     "iopub.status.idle": "2024-08-20T19:21:48.976701Z",
     "shell.execute_reply": "2024-08-20T19:21:48.975830Z"
    },
    "papermill": {
     "duration": 0.040976,
     "end_time": "2024-08-20T19:21:48.978707",
     "exception": false,
     "start_time": "2024-08-20T19:21:48.937731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ques = \"Name some Esports organization from Bangladesh.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a693bc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:49.046334Z",
     "iopub.status.busy": "2024-08-20T19:21:49.046063Z",
     "iopub.status.idle": "2024-08-20T19:21:51.412172Z",
     "shell.execute_reply": "2024-08-20T19:21:51.411231Z"
    },
    "papermill": {
     "duration": 2.403949,
     "end_time": "2024-08-20T19:21:51.415603",
     "exception": false,
     "start_time": "2024-08-20T19:21:49.011654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 10.1 seconds\n",
      "\n",
      "-----\n",
      "User:\n",
      " Name some Esports organization from Bangladesh.\n",
      "\n",
      "Raw Gemma response:\n",
      "\n",
      " <start_of_turn>user\n",
      "Context:  I'm familiar with a few esports organizations from Bangladesh. Here are a couple of notable ones:\n",
      "\n",
      "A1 Esports: A1 Esports is one of the prominent esports organizations in Bangladesh. They are well-known for their presence in competitive gaming, particularly in PUBG Mobile, where they've had significant success both locally and internationally.\n",
      "FS-Gaming (FsGSM): FS-Gaming is another well-known esports organization in Bangladesh. They have participated in various esports tournaments and are recognized in the local gaming community.\n",
      "These organizations are part of the growing esports scene in Bangladesh, which is gaining more attention and support over time.\n",
      "\n",
      "Question: Name some Esports organization from Bangladesh.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sure, here are the named Esports organization from Bangladesh in the text:\n",
      "\n",
      "- A1 Esports\n",
      "- FS-Gaming (FsGSM)\n",
      "\n",
      "\n",
      "Extracted Gemma response:\n",
      "\n",
      " Sure, here are the named Esports organization from Bangladesh in the text:\n",
      "\n",
      "- A1 Esports\n",
      "- FS-Gaming (FsGSM)\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt\n",
    "prompt = f\"\"\"<start_of_turn>user\n",
    "Context: {context}\n",
    "Question: {ques}<end_of_turn>\n",
    "<start_of_turn>model\n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "# Generate the outputs from prompt\n",
    "generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n",
    "# Decode the generated output\n",
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n",
    "                                     clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "# Extract the answer\n",
    "\n",
    "# Split and select the last item in the list\n",
    "gemma_response = response.split('<start_of_turn>model')[-1]\n",
    "# Remove leading and trailing spaces\n",
    "gemma_response = gemma_response.strip()\n",
    "# Remove the '<end_of_turn> token\n",
    "gemma_response= gemma_response.replace('<end_of_turn>', \"\")\n",
    "\n",
    "# Get the inference time\n",
    "elapsed_time = timer(start_time)\n",
    "print(f\"Time taken: {elapsed_time} seconds\")\n",
    "print()\n",
    "\n",
    "print('-----')\n",
    "print('User:\\n',ques)\n",
    "print()\n",
    "print('Raw Gemma response:\\n\\n',response)\n",
    "print()\n",
    "print()\n",
    "print('Extracted Gemma response:\\n\\n',gemma_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf771ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:51.485460Z",
     "iopub.status.busy": "2024-08-20T19:21:51.484502Z",
     "iopub.status.idle": "2024-08-20T19:21:51.491721Z",
     "shell.execute_reply": "2024-08-20T19:21:51.490867Z"
    },
    "papermill": {
     "duration": 0.044204,
     "end_time": "2024-08-20T19:21:51.493944",
     "exception": false,
     "start_time": "2024-08-20T19:21:51.449740",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hello_world(question, context):   \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"<start_of_turn>user\n",
    "    Context: {context}\n",
    "    Question: {question}<end_of_turn>\n",
    "    <start_of_turn>model\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    # Generate the outputs from prompt\n",
    "    generate_ids = gemma_model.generate(**inputs, max_new_tokens=768)\n",
    "    # Decode the generated output\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n",
    "                                         clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "    # Extract the answer\n",
    "\n",
    "    # Split and select the last item in the list\n",
    "    gemma_response = response.split('<start_of_turn>model')[-1]\n",
    "    # Remove leading and trailing spaces\n",
    "    gemma_response = gemma_response.strip()\n",
    "    # Remove the '<end_of_turn> token\n",
    "    gemma_response= gemma_response.replace('<end_of_turn>', \"\")\n",
    "    \n",
    "    # Clear the memory to create space\n",
    "    del prompt\n",
    "    del inputs\n",
    "    del generate_ids\n",
    "    torch.cuda.empty_cache() \n",
    "    gc.collect()\n",
    "        \n",
    "    return gemma_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da109544",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:51.565371Z",
     "iopub.status.busy": "2024-08-20T19:21:51.564684Z",
     "iopub.status.idle": "2024-08-20T19:21:51.571122Z",
     "shell.execute_reply": "2024-08-20T19:21:51.570143Z"
    },
    "papermill": {
     "duration": 0.042695,
     "end_time": "2024-08-20T19:21:51.573211",
     "exception": false,
     "start_time": "2024-08-20T19:21:51.530516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COVID-19, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly became a global pandemic. Characterized by respiratory symptoms, fever, and fatigue, the virus spread across the world, leading to millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n",
      "\n",
      "Whaat is covid 19?\n"
     ]
    }
   ],
   "source": [
    "context = f\"\"\"\n",
    "COVID-19, caused by the SARS-CoV-2 virus, emerged in late 2019 and rapidly became a global pandemic. Characterized by respiratory symptoms, fever, and fatigue, the virus spread across the world, leading to millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n",
    "\"\"\"\n",
    "ques = \"Whaat is covid 19?\"\n",
    "\n",
    "print(context)\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e7b57b86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:21:51.641028Z",
     "iopub.status.busy": "2024-08-20T19:21:51.640439Z",
     "iopub.status.idle": "2024-08-20T19:22:01.210875Z",
     "shell.execute_reply": "2024-08-20T19:22:01.209760Z"
    },
    "papermill": {
     "duration": 9.606583,
     "end_time": "2024-08-20T19:22:01.213007",
     "exception": false,
     "start_time": "2024-08-20T19:21:51.606424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whaat is covid 19?\n",
      "Sure, here is the answer to the question:\n",
      "\n",
      "COVID-19, caused by the SARS-CoV-2 virus, is a viral infection that emerged in late 2019 and quickly spread worldwide, becoming a global pandemic. It is characterized by respiratory symptoms, fever, and fatigue. The virus has caused millions of infections and significant mortality. Governments implemented lockdowns, social distancing, and mask mandates to curb the spread, while scientists raced to develop vaccines. The pandemic disrupted daily life, economies, and healthcare systems, highlighting the importance of public health infrastructure. Vaccination efforts and public health measures eventually helped control the virus, though its long-term impact continues to be felt globally.\n"
     ]
    }
   ],
   "source": [
    "ans = hello_world(ques, context)\n",
    "print(ques)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c721414e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:01.281772Z",
     "iopub.status.busy": "2024-08-20T19:22:01.281437Z",
     "iopub.status.idle": "2024-08-20T19:22:04.981724Z",
     "shell.execute_reply": "2024-08-20T19:22:04.980788Z"
    },
    "papermill": {
     "duration": 3.736838,
     "end_time": "2024-08-20T19:22:04.983865",
     "exception": false,
     "start_time": "2024-08-20T19:22:01.247027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give three topics from this context. Not any Description Just the topics.\n",
      "Sure, here are the three topics from the provided text:\n",
      "\n",
      "    1. COVID-19 pandemic and its impact on global health\n",
      "    2. Vaccination efforts and public health measures\n",
      "    3. The importance of public health infrastructure\n"
     ]
    }
   ],
   "source": [
    "ques = \"Give three topics from this context. Not any Description Just the topics.\"\n",
    "ans = hello_world(ques, context)\n",
    "print(ques)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffe85d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:05.053156Z",
     "iopub.status.busy": "2024-08-20T19:22:05.052857Z",
     "iopub.status.idle": "2024-08-20T19:22:05.056811Z",
     "shell.execute_reply": "2024-08-20T19:22:05.056006Z"
    },
    "papermill": {
     "duration": 0.040475,
     "end_time": "2024-08-20T19:22:05.058841",
     "exception": false,
     "start_time": "2024-08-20T19:22:05.018366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# context_text = f\"\"\"\n",
    "# A mother is often seen as the heart of the family, offering unconditional love, care, and support. She nurtures and guides her children, providing them with the foundation they need to grow into confident and compassionate individuals. A mother's role goes beyond just caregiving; she imparts values, offers wisdom, and serves as a source of comfort and strength throughout her children's lives. Her influence is profound, shaping not only the lives of her children but also the future of the family.\n",
    "# \"\"\"\n",
    "# ans = hello_world(ques, context_text)\n",
    "# print(ques)\n",
    "\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928075b4",
   "metadata": {
    "papermill": {
     "duration": 0.033124,
     "end_time": "2024-08-20T19:22:05.125603",
     "exception": false,
     "start_time": "2024-08-20T19:22:05.092479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Use the Chat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cbd1d549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:05.193315Z",
     "iopub.status.busy": "2024-08-20T19:22:05.193042Z",
     "iopub.status.idle": "2024-08-20T19:22:05.203216Z",
     "shell.execute_reply": "2024-08-20T19:22:05.202348Z"
    },
    "papermill": {
     "duration": 0.046129,
     "end_time": "2024-08-20T19:22:05.205141",
     "exception": false,
     "start_time": "2024-08-20T19:22:05.159012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>author</th>\n",
       "      <th>message</th>\n",
       "      <th>video_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-08-12 09:22:22</td>\n",
       "      <td>Rizvan</td>\n",
       "      <td>hii</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-08-12 09:22:35</td>\n",
       "      <td>Rizvan</td>\n",
       "      <td>#2d</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-08-12 09:25:29</td>\n",
       "      <td>RN KAKASHI</td>\n",
       "      <td>3rd</td>\n",
       "      <td>oiqpD3C_dLo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime      author message     video_id\n",
       "0  2022-08-12 09:22:22      Rizvan     hii  oiqpD3C_dLo\n",
       "1  2022-08-12 09:22:35      Rizvan     #2d  oiqpD3C_dLo\n",
       "2  2022-08-12 09:25:29  RN KAKASHI     3rd  oiqpD3C_dLo"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b015de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:05.274891Z",
     "iopub.status.busy": "2024-08-20T19:22:05.274634Z",
     "iopub.status.idle": "2024-08-20T19:22:05.324063Z",
     "shell.execute_reply": "2024-08-20T19:22:05.322956Z"
    },
    "papermill": {
     "duration": 0.086571,
     "end_time": "2024-08-20T19:22:05.326040",
     "exception": false,
     "start_time": "2024-08-20T19:22:05.239469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1 ❤️ A1 ❤️\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "def replace_emoji_codes(text):\n",
    "    \"\"\"\n",
    "    Replace text-based emoji codes with actual emojis using the `emoji` library.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text containing emoji codes.\n",
    "\n",
    "    Returns:\n",
    "    - str: The text with emoji codes replaced by actual emojis.\n",
    "    \"\"\"\n",
    "    # Convert text-based emoji codes to actual emojis\n",
    "    return emoji.emojize(text)\n",
    "\n",
    "# Example usage\n",
    "text = \"A1 :red_heart: A1 :red_heart:\"\n",
    "converted_text = replace_emoji_codes(text)\n",
    "print(converted_text)  # Output: \"A1 ❤️ A1 ❤️\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f658202b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:05.394848Z",
     "iopub.status.busy": "2024-08-20T19:22:05.394547Z",
     "iopub.status.idle": "2024-08-20T19:22:35.593702Z",
     "shell.execute_reply": "2024-08-20T19:22:35.592667Z"
    },
    "papermill": {
     "duration": 30.235621,
     "end_time": "2024-08-20T19:22:35.595591",
     "exception": false,
     "start_time": "2024-08-20T19:22:05.359970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2796193/2796193 [00:30<00:00, 92803.01it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "df['message'] = df.message.progress_apply(replace_emoji_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "552a52cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:35.708148Z",
     "iopub.status.busy": "2024-08-20T19:22:35.707340Z",
     "iopub.status.idle": "2024-08-20T19:22:35.713188Z",
     "shell.execute_reply": "2024-08-20T19:22:35.712246Z"
    },
    "papermill": {
     "duration": 0.063845,
     "end_time": "2024-08-20T19:22:35.715010",
     "exception": false,
     "start_time": "2024-08-20T19:22:35.651165",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2796193"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a74ead86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:35.826542Z",
     "iopub.status.busy": "2024-08-20T19:22:35.826281Z",
     "iopub.status.idle": "2024-08-20T19:22:35.830371Z",
     "shell.execute_reply": "2024-08-20T19:22:35.829527Z"
    },
    "papermill": {
     "duration": 0.061436,
     "end_time": "2024-08-20T19:22:35.832168",
     "exception": false,
     "start_time": "2024-08-20T19:22:35.770732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # filt = df[:10].copy()\n",
    "\n",
    "fkchat = df['message']\n",
    "\n",
    "# demo1 = fkchat[:100]\n",
    "# msg = \", \".join(demo1.astype(str))\n",
    "# msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dc40719",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:35.958112Z",
     "iopub.status.busy": "2024-08-20T19:22:35.957774Z",
     "iopub.status.idle": "2024-08-20T19:22:35.961894Z",
     "shell.execute_reply": "2024-08-20T19:22:35.961006Z"
    },
    "papermill": {
     "duration": 0.075405,
     "end_time": "2024-08-20T19:22:35.963857",
     "exception": false,
     "start_time": "2024-08-20T19:22:35.888452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# context = msg\n",
    "# # print(context)\n",
    "\n",
    "# ques = \"These are some live Chats from a Esports Live. Now Give me 2 short key topics from these chats. No Description just the topic please. Nothing else just the two topic name.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d582d250",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.086236Z",
     "iopub.status.busy": "2024-08-20T19:22:36.085666Z",
     "iopub.status.idle": "2024-08-20T19:22:36.090598Z",
     "shell.execute_reply": "2024-08-20T19:22:36.089456Z"
    },
    "papermill": {
     "duration": 0.072726,
     "end_time": "2024-08-20T19:22:36.093444",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.020718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ans = hello_world(ques, context)\n",
    "\n",
    "# print(ques, end='\\n')\n",
    "\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7010fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.211458Z",
     "iopub.status.busy": "2024-08-20T19:22:36.211187Z",
     "iopub.status.idle": "2024-08-20T19:22:36.214930Z",
     "shell.execute_reply": "2024-08-20T19:22:36.214047Z"
    },
    "papermill": {
     "duration": 0.061612,
     "end_time": "2024-08-20T19:22:36.216853",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.155241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = 100\n",
    "# demo1 = fkchat[x:x+100]\n",
    "# msg = \", \".join(demo1.astype(str))\n",
    "# msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe312107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.329754Z",
     "iopub.status.busy": "2024-08-20T19:22:36.329235Z",
     "iopub.status.idle": "2024-08-20T19:22:36.333036Z",
     "shell.execute_reply": "2024-08-20T19:22:36.332173Z"
    },
    "papermill": {
     "duration": 0.063018,
     "end_time": "2024-08-20T19:22:36.335002",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.271984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# context = msg\n",
    "# ans = hello_world(ques, context)\n",
    "\n",
    "# print(ques, end='\\n')\n",
    "\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68ff68dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.446413Z",
     "iopub.status.busy": "2024-08-20T19:22:36.446138Z",
     "iopub.status.idle": "2024-08-20T19:22:36.449619Z",
     "shell.execute_reply": "2024-08-20T19:22:36.448798Z"
    },
    "papermill": {
     "duration": 0.061517,
     "end_time": "2024-08-20T19:22:36.451458",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.389941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = 200\n",
    "# demo1 = fkchat[x:x+100]\n",
    "# msg = \", \".join(demo1.astype(str))\n",
    "# context = msg\n",
    "# ans = hello_world(ques, context)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76132fa1",
   "metadata": {
    "papermill": {
     "duration": 0.05486,
     "end_time": "2024-08-20T19:22:36.562877",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.508017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Apply it in a Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77af9d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.673528Z",
     "iopub.status.busy": "2024-08-20T19:22:36.673258Z",
     "iopub.status.idle": "2024-08-20T19:22:36.677646Z",
     "shell.execute_reply": "2024-08-20T19:22:36.676752Z"
    },
    "papermill": {
     "duration": 0.062011,
     "end_time": "2024-08-20T19:22:36.679509",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.617498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x = 0\n",
    "# chunk_size = 100\n",
    "# results = []\n",
    "\n",
    "# # lim = len(fkchat)\n",
    "# lim = 1000\n",
    "\n",
    "# total_iterations = (lim - x) // chunk_size\n",
    "\n",
    "# # Loop through the DataFrame in chunks of 100 with a progress bar\n",
    "# for _ in tqdm(range(total_iterations), desc=\"Processing\"):\n",
    "#     # Get the next 100 messages\n",
    "#     demo1 = fkchat[x:x + chunk_size]\n",
    "    \n",
    "#     # Convert the messages to a single string, separated by commas\n",
    "#     msg = \", \".join(demo1.astype(str))\n",
    "    \n",
    "#     # Pass the context to the hello_world function and store the result\n",
    "#     ans = hello_world(ques, msg)\n",
    "#     lines = ans.splitlines()\n",
    "\n",
    "#     # Rejoin the lines, skipping the first one\n",
    "#     new_text = \"\\n\".join(lines[1:])\n",
    "\n",
    "#     results.append(new_text)\n",
    "    \n",
    "#     # Move to the next chunk\n",
    "#     x += chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c16ca740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.790792Z",
     "iopub.status.busy": "2024-08-20T19:22:36.790348Z",
     "iopub.status.idle": "2024-08-20T19:22:36.793736Z",
     "shell.execute_reply": "2024-08-20T19:22:36.792927Z"
    },
    "papermill": {
     "duration": 0.060895,
     "end_time": "2024-08-20T19:22:36.795567",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.734672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for v in results:\n",
    "#     print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed58ac53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:36.906910Z",
     "iopub.status.busy": "2024-08-20T19:22:36.906591Z",
     "iopub.status.idle": "2024-08-20T19:22:36.910426Z",
     "shell.execute_reply": "2024-08-20T19:22:36.909639Z"
    },
    "papermill": {
     "duration": 0.061243,
     "end_time": "2024-08-20T19:22:36.912266",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.851023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d47e2034",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:37.026518Z",
     "iopub.status.busy": "2024-08-20T19:22:37.025969Z",
     "iopub.status.idle": "2024-08-20T19:22:37.031735Z",
     "shell.execute_reply": "2024-08-20T19:22:37.030902Z"
    },
    "papermill": {
     "duration": 0.064738,
     "end_time": "2024-08-20T19:22:37.033787",
     "exception": false,
     "start_time": "2024-08-20T19:22:36.969049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Soul se. nehi ho payega 🤣', 'Ste and A1 sinister']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\\n    - Soul se. nehi ho payega 🤣\\n    - Ste and A1 sinister\"\n",
    "\n",
    "# Split the text into separate lines based on newline\n",
    "separated_texts = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "# Remove leading hyphens, numbers, or other unwanted characters\n",
    "cleaned_texts = [re.sub(r'^[\\d\\-\\s]+', '', line) for line in separated_texts]\n",
    "\n",
    "# Now you have cleaned lines as separate values in a list\n",
    "print(cleaned_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f030c2b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:37.146977Z",
     "iopub.status.busy": "2024-08-20T19:22:37.146538Z",
     "iopub.status.idle": "2024-08-20T19:22:37.153008Z",
     "shell.execute_reply": "2024-08-20T19:22:37.152166Z"
    },
    "papermill": {
     "duration": 0.064676,
     "end_time": "2024-08-20T19:22:37.154763",
     "exception": false,
     "start_time": "2024-08-20T19:22:37.090087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def cln_texts(text):\n",
    "    # Split the text into separate lines based on newline\n",
    "    separated_texts = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "    # Remove leading hyphens, numbers, or other unwanted characters\n",
    "    cleaned_texts = [re.sub(r'^[\\d\\-\\s]+', '', line) for line in separated_texts]\n",
    "\n",
    "    # Remove punctuation\n",
    "    cleaned_texts = [line.translate(str.maketrans('', '', string.punctuation)) for line in cleaned_texts]\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    cleaned_texts = [re.sub(r'\\s+', ' ', line).strip() for line in cleaned_texts]\n",
    "    \n",
    "    return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee9d4613",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:37.266184Z",
     "iopub.status.busy": "2024-08-20T19:22:37.265899Z",
     "iopub.status.idle": "2024-08-20T19:22:37.269640Z",
     "shell.execute_reply": "2024-08-20T19:22:37.268776Z"
    },
    "papermill": {
     "duration": 0.061722,
     "end_time": "2024-08-20T19:22:37.271469",
     "exception": false,
     "start_time": "2024-08-20T19:22:37.209747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# farr = []\n",
    "# for v in tqdm(results):\n",
    "#     farr.append(cln_texts(v))\n",
    "    \n",
    "# farr = [item for sublist in farr for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "918b1896",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:37.383252Z",
     "iopub.status.busy": "2024-08-20T19:22:37.383003Z",
     "iopub.status.idle": "2024-08-20T19:22:37.387445Z",
     "shell.execute_reply": "2024-08-20T19:22:37.386782Z"
    },
    "papermill": {
     "duration": 0.062475,
     "end_time": "2024-08-20T19:22:37.389257",
     "exception": false,
     "start_time": "2024-08-20T19:22:37.326782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# farr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219bdb2",
   "metadata": {
    "papermill": {
     "duration": 0.054843,
     "end_time": "2024-08-20T19:22:37.498994",
     "exception": false,
     "start_time": "2024-08-20T19:22:37.444151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Now Apply for big chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e26bdb06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T19:22:37.610549Z",
     "iopub.status.busy": "2024-08-20T19:22:37.610277Z",
     "iopub.status.idle": "2024-08-20T20:34:18.859921Z",
     "shell.execute_reply": "2024-08-20T20:34:18.858941Z"
    },
    "papermill": {
     "duration": 4301.307596,
     "end_time": "2024-08-20T20:34:18.862092",
     "exception": false,
     "start_time": "2024-08-20T19:22:37.554496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 900/900 [1:11:41<00:00,  4.78s/it]\n"
     ]
    }
   ],
   "source": [
    "x = 100_000\n",
    "chunk_size = 100\n",
    "ult_results = []\n",
    "\n",
    "# lim = len(fkchat)\n",
    "lim = x+90_000\n",
    "\n",
    "total_iterations = (lim - x) // chunk_size\n",
    "\n",
    "# Loop through the DataFrame in chunks of 100 with a progress bar\n",
    "for _ in tqdm(range(total_iterations), desc=\"Processing\"):\n",
    "    # Get the next 100 messages\n",
    "    demo1 = fkchat[x:x + chunk_size]\n",
    "    \n",
    "    # Convert the messages to a single string, separated by commas\n",
    "    msg = \", \".join(demo1.astype(str))\n",
    "    \n",
    "    try:\n",
    "        # Pass the context to the hello_world function and store the result\n",
    "        ans = hello_world(ques, msg)\n",
    "        lines = ans.splitlines()\n",
    "\n",
    "        # Rejoin the lines, skipping the first one\n",
    "        new_text = \"\\n\".join(lines[1:])\n",
    "\n",
    "        ult_results.append(new_text)\n",
    "    except:\n",
    "        ult_results.append('None')\n",
    "\n",
    "    \n",
    "    # Move to the next chunk\n",
    "    x += chunk_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fad728c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:19.115396Z",
     "iopub.status.busy": "2024-08-20T20:34:19.115076Z",
     "iopub.status.idle": "2024-08-20T20:34:19.119111Z",
     "shell.execute_reply": "2024-08-20T20:34:19.118221Z"
    },
    "papermill": {
     "duration": 0.132597,
     "end_time": "2024-08-20T20:34:19.120957",
     "exception": false,
     "start_time": "2024-08-20T20:34:18.988360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ult_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d89c7a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:19.381483Z",
     "iopub.status.busy": "2024-08-20T20:34:19.380830Z",
     "iopub.status.idle": "2024-08-20T20:34:19.419883Z",
     "shell.execute_reply": "2024-08-20T20:34:19.418973Z"
    },
    "papermill": {
     "duration": 0.170864,
     "end_time": "2024-08-20T20:34:19.421951",
     "exception": false,
     "start_time": "2024-08-20T20:34:19.251087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 900/900 [00:00<00:00, 29660.36it/s]\n"
     ]
    }
   ],
   "source": [
    "farr = []\n",
    "for v in tqdm(ult_results):\n",
    "    farr.append(cln_texts(v))\n",
    "    \n",
    "farr = [item for sublist in farr for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b3003a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:19.675484Z",
     "iopub.status.busy": "2024-08-20T20:34:19.675203Z",
     "iopub.status.idle": "2024-08-20T20:34:19.680949Z",
     "shell.execute_reply": "2024-08-20T20:34:19.680121Z"
    },
    "papermill": {
     "duration": 0.133996,
     "end_time": "2024-08-20T20:34:19.682781",
     "exception": false,
     "start_time": "2024-08-20T20:34:19.548785",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gaming community in Bangladesh',\n",
       " 'International tournament participation',\n",
       " 'Support for Pakistani team',\n",
       " 'Gaming and Esports',\n",
       " 'Sports and Support',\n",
       " 'Miscellaneous Discussions',\n",
       " 'Support Saudi Arabia Falcon Oppp',\n",
       " 'GSM the most aggressive squad in this Milky Way',\n",
       " 'Esports',\n",
       " 'India vs Bangladesh in the 2022 Champions Trophy']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f373fd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:19.937736Z",
     "iopub.status.busy": "2024-08-20T20:34:19.937438Z",
     "iopub.status.idle": "2024-08-20T20:34:19.942801Z",
     "shell.execute_reply": "2024-08-20T20:34:19.941916Z"
    },
    "papermill": {
     "duration": 0.135936,
     "end_time": "2024-08-20T20:34:19.945028",
     "exception": false,
     "start_time": "2024-08-20T20:34:19.809092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2701, 1)\n"
     ]
    }
   ],
   "source": [
    "topic_df = pd.DataFrame({'topic': farr})\n",
    "\n",
    "print(topic_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7da86b00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:20.200818Z",
     "iopub.status.busy": "2024-08-20T20:34:20.200425Z",
     "iopub.status.idle": "2024-08-20T20:34:20.210092Z",
     "shell.execute_reply": "2024-08-20T20:34:20.209208Z"
    },
    "papermill": {
     "duration": 0.140444,
     "end_time": "2024-08-20T20:34:20.212246",
     "exception": false,
     "start_time": "2024-08-20T20:34:20.071802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaming community in Bangladesh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>International tournament participation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Support for Pakistani team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gaming and Esports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sports and Support</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    topic\n",
       "0          Gaming community in Bangladesh\n",
       "1  International tournament participation\n",
       "2              Support for Pakistani team\n",
       "3                      Gaming and Esports\n",
       "4                      Sports and Support"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c060944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:20.469731Z",
     "iopub.status.busy": "2024-08-20T20:34:20.469081Z",
     "iopub.status.idle": "2024-08-20T20:34:20.487263Z",
     "shell.execute_reply": "2024-08-20T20:34:20.486394Z"
    },
    "papermill": {
     "duration": 0.149952,
     "end_time": "2024-08-20T20:34:20.489230",
     "exception": false,
     "start_time": "2024-08-20T20:34:20.339278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_df.to_csv('All_topicsp2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f60f5",
   "metadata": {
    "papermill": {
     "duration": 0.125241,
     "end_time": "2024-08-20T20:34:20.741885",
     "exception": false,
     "start_time": "2024-08-20T20:34:20.616644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "382410bf",
   "metadata": {
    "papermill": {
     "duration": 0.125724,
     "end_time": "2024-08-20T20:34:20.994393",
     "exception": false,
     "start_time": "2024-08-20T20:34:20.868669",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Shorten The Topic List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68faa10d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:21.253575Z",
     "iopub.status.busy": "2024-08-20T20:34:21.252767Z",
     "iopub.status.idle": "2024-08-20T20:34:21.257524Z",
     "shell.execute_reply": "2024-08-20T20:34:21.256672Z"
    },
    "papermill": {
     "duration": 0.138633,
     "end_time": "2024-08-20T20:34:21.259735",
     "exception": false,
     "start_time": "2024-08-20T20:34:21.121102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2701\n"
     ]
    }
   ],
   "source": [
    "print(len(farr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "13d93445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:21.519070Z",
     "iopub.status.busy": "2024-08-20T20:34:21.518407Z",
     "iopub.status.idle": "2024-08-20T20:34:21.522507Z",
     "shell.execute_reply": "2024-08-20T20:34:21.521657Z"
    },
    "papermill": {
     "duration": 0.135663,
     "end_time": "2024-08-20T20:34:21.524485",
     "exception": false,
     "start_time": "2024-08-20T20:34:21.388822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics = \", \".join(farr[:100])\n",
    "# topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1b197ff8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:21.779933Z",
     "iopub.status.busy": "2024-08-20T20:34:21.779665Z",
     "iopub.status.idle": "2024-08-20T20:34:21.783488Z",
     "shell.execute_reply": "2024-08-20T20:34:21.782664Z"
    },
    "papermill": {
     "duration": 0.134317,
     "end_time": "2024-08-20T20:34:21.785535",
     "exception": false,
     "start_time": "2024-08-20T20:34:21.651218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ques = \"There are similar type of topics in this context list. From these topics give me top 5 topics.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40bbbcf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:22.041721Z",
     "iopub.status.busy": "2024-08-20T20:34:22.041118Z",
     "iopub.status.idle": "2024-08-20T20:34:22.045127Z",
     "shell.execute_reply": "2024-08-20T20:34:22.044134Z"
    },
    "papermill": {
     "duration": 0.133696,
     "end_time": "2024-08-20T20:34:22.047078",
     "exception": false,
     "start_time": "2024-08-20T20:34:21.913382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ans = hello_world(ques, topics)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9341ba1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:22.302860Z",
     "iopub.status.busy": "2024-08-20T20:34:22.302146Z",
     "iopub.status.idle": "2024-08-20T20:34:22.306292Z",
     "shell.execute_reply": "2024-08-20T20:34:22.305334Z"
    },
    "papermill": {
     "duration": 0.133991,
     "end_time": "2024-08-20T20:34:22.308268",
     "exception": false,
     "start_time": "2024-08-20T20:34:22.174277",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# topics = \", \".join(farr[100:])\n",
    "# ans = hello_world(ques, topics)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87f4c1a",
   "metadata": {
    "papermill": {
     "duration": 0.126857,
     "end_time": "2024-08-20T20:34:22.563378",
     "exception": false,
     "start_time": "2024-08-20T20:34:22.436521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afcfc853",
   "metadata": {
    "papermill": {
     "duration": 0.128232,
     "end_time": "2024-08-20T20:34:22.819735",
     "exception": false,
     "start_time": "2024-08-20T20:34:22.691503",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Few Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9a8d7c11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:23.074318Z",
     "iopub.status.busy": "2024-08-20T20:34:23.073957Z",
     "iopub.status.idle": "2024-08-20T20:34:23.078462Z",
     "shell.execute_reply": "2024-08-20T20:34:23.077600Z"
    },
    "papermill": {
     "duration": 0.133191,
     "end_time": "2024-08-20T20:34:23.080371",
     "exception": false,
     "start_time": "2024-08-20T20:34:22.947180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the few shot data into a pandas dataframe\n",
    "# df_fshot = pd.read_csv(FEW_SHOT_DATA_PATH)\n",
    "\n",
    "# def convert_to_list(x):\n",
    "    \n",
    "#     # Convert the string to a list: '[...]' to [...]\n",
    "#     x_as_list = ast.literal_eval(x)\n",
    "    \n",
    "#     return x_as_list\n",
    "\n",
    "# # Convert each item in the context column from a string to a \n",
    "# # python list i.e. '[...]' to [...]\n",
    "# df_fshot['gem_context'] = df_fshot['gem_context'].apply(convert_to_list)\n",
    "\n",
    "# df_fshot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74f225d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:23.336740Z",
     "iopub.status.busy": "2024-08-20T20:34:23.336413Z",
     "iopub.status.idle": "2024-08-20T20:34:23.340267Z",
     "shell.execute_reply": "2024-08-20T20:34:23.339406Z"
    },
    "papermill": {
     "duration": 0.133611,
     "end_time": "2024-08-20T20:34:23.342048",
     "exception": false,
     "start_time": "2024-08-20T20:34:23.208437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(df_fshot.shape)\n",
    "# df_fshot.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "696a6558",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:23.597546Z",
     "iopub.status.busy": "2024-08-20T20:34:23.597175Z",
     "iopub.status.idle": "2024-08-20T20:34:23.602734Z",
     "shell.execute_reply": "2024-08-20T20:34:23.601871Z"
    },
    "papermill": {
     "duration": 0.135593,
     "end_time": "2024-08-20T20:34:23.605308",
     "exception": false,
     "start_time": "2024-08-20T20:34:23.469715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def hello_world_again(question, context):   \n",
    "#     # Create the prompt\n",
    "#     prompt = f\"\"\"<start_of_turn>user\n",
    "#     Context: {df_fshot.loc[0, 'gem_context']}\n",
    "#     Question: {df_fshot.loc[0, 'query']}<end_of_turn>\n",
    "#     <start_of_turn>model\n",
    "#     {df_fshot.loc[0, 'corrected_text']}<end_of_turn>\n",
    "#     <start_of_turn>user\n",
    "#     Context: {df_fshot.loc[5, 'gem_context']}\n",
    "#     Question: {df_fshot.loc[5, 'query']}<end_of_turn>\n",
    "#     <start_of_turn>model\n",
    "#     {df_fshot.loc[5, 'corrected_text']}<end_of_turn>\n",
    "#     <start_of_turn>user\n",
    "#     Context: {df_fshot.loc[6, 'gem_context']}\n",
    "#     Question: {df_fshot.loc[6, 'query']}<end_of_turn>\n",
    "#     <start_of_turn>model\n",
    "#     {df_fshot.loc[6, 'corrected_text']}<end_of_turn>\n",
    "#     <start_of_turn>user\n",
    "#     Think and write your step-by-step reasoning before responding.\n",
    "    \n",
    "#     Context: {context}\n",
    "#     Question: {question}<end_of_turn>\n",
    "#     <start_of_turn>model\n",
    "#     \"\"\"\n",
    "\n",
    "\n",
    "#     # Tokenize the prompt\n",
    "#     inp = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "#     # Generate the outputs from prompt\n",
    "#     generate_ids = gemma_model.generate(**inp, max_new_tokens=768)\n",
    "#     # Decode the generated output\n",
    "#     response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True,\n",
    "#                                          clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "#     # Extract the answer\n",
    "\n",
    "#     # Split and select the last item in the list\n",
    "#     gemma_response = response.split('<start_of_turn>model')[-1]\n",
    "#     # Remove leading and trailing spaces\n",
    "#     gemma_response = gemma_response.strip()\n",
    "#     # Remove the '<end_of_turn> token\n",
    "#     gemma_response= gemma_response.replace('<end_of_turn>', \"\")\n",
    "    \n",
    "#     return gemma_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0dbbf0e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:23.864567Z",
     "iopub.status.busy": "2024-08-20T20:34:23.863713Z",
     "iopub.status.idle": "2024-08-20T20:34:23.867624Z",
     "shell.execute_reply": "2024-08-20T20:34:23.866908Z"
    },
    "papermill": {
     "duration": 0.134586,
     "end_time": "2024-08-20T20:34:23.869514",
     "exception": false,
     "start_time": "2024-08-20T20:34:23.734928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ans = hello_world_again(ques, context)\n",
    "# print(ques)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "154ad41d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-20T20:34:24.173739Z",
     "iopub.status.busy": "2024-08-20T20:34:24.173293Z",
     "iopub.status.idle": "2024-08-20T20:34:24.177295Z",
     "shell.execute_reply": "2024-08-20T20:34:24.176446Z"
    },
    "papermill": {
     "duration": 0.136122,
     "end_time": "2024-08-20T20:34:24.179274",
     "exception": false,
     "start_time": "2024-08-20T20:34:24.043152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ans = hello_world_again(ques, context_text)\n",
    "# print(ques)\n",
    "# print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984862af",
   "metadata": {
    "papermill": {
     "duration": 0.12679,
     "end_time": "2024-08-20T20:34:24.432714",
     "exception": false,
     "start_time": "2024-08-20T20:34:24.305924",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4477853,
     "sourceId": 7918995,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5528375,
     "sourceId": 9181722,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8332,
     "sourceId": 11261,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4607.64154,
   "end_time": "2024-08-20T20:34:27.596822",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-08-20T19:17:39.955282",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0bd1bb9c4aea4e55b193a769868bc0b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "143f5e88aa084fcebc06a12583635229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "1fd952b0e3e042529e081813c654b9a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2c60a029d7164000af81086a79f32a75",
       "placeholder": "​",
       "style": "IPY_MODEL_2673ae3fdd5a42e7a9048053dbc5e66a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "2673ae3fdd5a42e7a9048053dbc5e66a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "2c60a029d7164000af81086a79f32a75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "530a9cdb81714018a3f22c4f1bb8a5c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_74d586da765e4fcbb093ef80b2fdf43e",
       "max": 4.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_143f5e88aa084fcebc06a12583635229",
       "value": 4.0
      }
     },
     "74d586da765e4fcbb093ef80b2fdf43e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "892b46649ae34af69c6a94686de58a78": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8d575ebea65444cdb3c66cedcbe3d1cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b1dbaff1845a4fdfa8c23d7e7b3af47a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1fd952b0e3e042529e081813c654b9a6",
        "IPY_MODEL_530a9cdb81714018a3f22c4f1bb8a5c7",
        "IPY_MODEL_c04753ddbd0e4502b816eb25cc8006d6"
       ],
       "layout": "IPY_MODEL_892b46649ae34af69c6a94686de58a78"
      }
     },
     "c04753ddbd0e4502b816eb25cc8006d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0bd1bb9c4aea4e55b193a769868bc0b7",
       "placeholder": "​",
       "style": "IPY_MODEL_8d575ebea65444cdb3c66cedcbe3d1cb",
       "value": " 4/4 [01:53&lt;00:00, 25.78s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
